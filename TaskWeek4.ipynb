{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyelyra/TaskWeek4/blob/main/TaskWeek4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nama: Anyelyra Kantata\n",
        "\n",
        "NPM: 2206048625"
      ],
      "metadata": {
        "id": "eevzw7p_Xp2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsLfysWAEkZJ",
        "outputId": "71056f5f-db97-4940-f312-837d764f8f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l==1.0.3 in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (3.7.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (2.31.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (1.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (5.6.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (10.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline==0.1.6->d2l==1.0.3) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l==1.0.3) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.6.9)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.0.13)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (2.18.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.1.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter==1.0.0->d2l==1.0.3) (2.4.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.23.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l==1.0.3) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l==1.0.3) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LLHUul_xDndj"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.1. Deep Convolutional Neural Networks (AlexNet)"
      ],
      "metadata": {
        "id": "tex36BqZaAWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.2. AlexNet"
      ],
      "metadata": {
        "id": "qozYhteMaLsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.2.3. Capacity Control and Preprocessing|"
      ],
      "metadata": {
        "id": "zUUqfYRtaV7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "TAvDq2YAZ_dQ",
        "outputId": "39789c0d-73fa-4eb7-a04c-e67bb54fbd1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'd2l' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4c2dd943a100>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAlexNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.net = nn.Sequential(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'd2l' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AlexNet().layer_summary((1, 1, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIYa_XA4aa5W",
        "outputId": "27dbecd7-cc63-44df-e98b-89cbae5317bd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
            "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
            "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
            "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
            "Flatten output shape:\t torch.Size([1, 6400])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.3. Training"
      ],
      "metadata": {
        "id": "1JhHK_Jdad3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet(lr=0.01)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "ncvtmuSWagqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.5. Exercises"
      ],
      "metadata": {
        "id": "_egnVSt8ajDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Following up on the discussion above, analyze the computational properties of AlexNet."
      ],
      "metadata": {
        "id": "JZt-73Zza4JH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1. Compute the memory footprint for convolutions and fully connected layers, respectively. Which one dominates?"
      ],
      "metadata": {
        "id": "DrMYVwQIbCOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Alexnet()\n",
        "X = torch.randn(1,3, 224, 224)\n",
        "_ = model(X)\n",
        "params = {'conv':0, 'lr':0}\n",
        "for idx, module in enumerate(model.net):\n",
        "    if type(module) not in (nn.Linear,nn.Conv2d):\n",
        "        continue\n",
        "    num = sum(p.numel() for p in module.parameters())\n",
        "    # print(f\"Module {idx + 1}: {num} parameters type:{type(module)}\")\n",
        "    if type(module) == nn.Conv2d:\n",
        "        params['conv'] += num\n",
        "\n",
        "    else:\n",
        "        params['lr'] += num\n",
        "\n",
        "params"
      ],
      "metadata": {
        "id": "l-NVs91UbOc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 2. Calculate the computational cost for the convolutions and the fully connected layers."
      ],
      "metadata": {
        "id": "ABZ8woSTb2eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1,3, 224, 224)\n",
        "params = {'conv':0, 'lr':0}\n",
        "for idx, module in enumerate(model.net):\n",
        "    c_i = x.shape[1]\n",
        "    x = module(x)\n",
        "    if type(module) == nn.Conv2d:\n",
        "        k = [p.shape for p in module.parameters()]\n",
        "        c_o,h_o,w_o = x.shape[1], x.shape[2], x.shape[3]\n",
        "        params['conv'] += c_i*c_o*h_o*w_o*k[0][-1]*k[0][-2]\n",
        "    if type(module) == nn.Linear:\n",
        "        params['lr'] += sum(p.numel() for p in module.parameters())\n",
        "params"
      ],
      "metadata": {
        "id": "FkMBor-Xb8et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(1,3, 224, 224)\n",
        "_ = model(X)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)"
      ],
      "metadata": {
        "id": "OlbRP8TucEE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 3. How does the memory (read and write bandwidth, latency, size) affect computation? Is there any difference in its effects for training and inference?"
      ],
      "metadata": {
        "id": "MzmZqQJ3b9aC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bandwidth baca dan tulis, latency, dan ukuran mesangat mempengaruhi komputasi dalam pelatihan dan inferensi neural network.\n",
        "\n",
        "* Bandwidth yang tinggi memungkinkan perpindahan data lebih cepat antara memori dan unit pemrosesan, bandwith yang tinggi juga mempercepat pelatihan dan inferensi karena mengurangi waktu menunggu data.\n",
        "\n",
        "* Latency rendah mempercepat akses data dan komputasi. Pada training, latency rendah penting untuk pembaruan bobot dan gradien, sementara dalam inferensi penting untuk memastikan respon yang cepat, terutama di aplikasi yang real time.\n",
        "\n",
        "* Ukuran memori yang besar memungkinkan penyimpanan data lebih banyak. Dalam pelatihan, ini membantu menampung gradien dan parameter model, sedangkan dalam inferensi memori yang cukup membantu menyimpan hasil perantara."
      ],
      "metadata": {
        "id": "6KkYSVyacG5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. You are a chip designer and need to trade off computation and memory bandwidth. For example, a faster chip requires more power and possibly a larger chip area. More memory bandwidth requires more pins and control logic, thus also more area. How do you optimize?"
      ],
      "metadata": {
        "id": "nDizHMmidCxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Menentukan tujuan kerja\n",
        "* Menganalisis pola akses memori dan komputasi.\n",
        "* Seimbangkan unit komputasi dan memori dengan menyesuaikan jumlah inti, ukuran cache, dan hirarki memori.\n",
        "* Buat hirarki memori yang efisien untuk mengurangi bottleneck.\n",
        "* Tingkatkan bandwidth memori dan gunakan paralelisme untuk meningkatkan efisiensi.\n",
        "* Evaluasi dan sesuaikan desain berdasarkan hasil simulasi dan umpan balik."
      ],
      "metadata": {
        "id": "8TkamVdxdGap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do engineers no longer report performance benchmarks on AlexNet?"
      ],
      "metadata": {
        "id": "5ATbVt_bdaYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tidak lagi mencerminkan efisiensi dan akurasi model terkini.\n",
        "* Model modern seperti VGG, ResNet, dan model berbasis Transformer telah lebih populer dan efisien.\n",
        "* Engineer ;ebih memilih model yang disesuaikan untuk tugas tertentu, seperti EfficientNet untuk klasifikasi gambar.\n",
        "* Kinerja dalam aplikasi nyata lebih diutamakan, seperti analisis citra medis dan pemahaman bahasa alami."
      ],
      "metadata": {
        "id": "eAF_Y13jdeUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Try increasing the number of epochs when training AlexNet. Compared with LeNet, how do the results differ? Why?"
      ],
      "metadata": {
        "id": "0p-edUVUduEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AlexNet menunjukkan peningkatan akurasi dan ketahanan dalam mengklasifikasikan pola yang kompleks karena arsitekturnya yang lebih dalam dan jumlah parameter yang lebih besar, yang memungkinkannya untuk mempelajari fitur-fitur yang lebih rumit dari dataset.\n",
        "\n",
        "Sedangkan, LeNet mungkin menunjukkan sedikit peningkatan dengan bertambahnya epoch, arsitektur yang lebih sederhana membatasi kemampuannya untuk menangkap tingkat detail yang sama seperti AlexNet.\n",
        "\n",
        "Sehingga, AlexNet lebih efektif untuk dataset yang lebih besar dan tugas yang lebih menantang, sehingga menghasilkan kinerja yang lebih baik seiring dengan peningkatan jumlah epoch pelatihan.\n",
        "\n",
        "Perilaku ini disebabkan kemampuan jaringan yang lebih dalam untuk memanfaatkan data dengan lebih efektif, sehingga meningkatkan kemampuan generalisasi sambil meminimalkan risiko overfitting."
      ],
      "metadata": {
        "id": "gZUFSmUHd0PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. AlexNet may be too complex for the Fashion-MNIST dataset, in particular due to the low resolution of the initial images."
      ],
      "metadata": {
        "id": "QraTrNrTeJEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1. Try simplifying the model to make the training faster, while ensuring that the accuracy does not drop significantly."
      ],
      "metadata": {
        "id": "_qePRSUoeL__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langakah-langkah:\n",
        "1. Mengurangi jumlah layer, ukuran filter, dan dimensi input\n",
        "2. Menggunakan pengaturan training yang berbeda\n",
        "3. Menggunakan regularization\n",
        "4. Menggunakan data augmentation"
      ],
      "metadata": {
        "id": "SLxGw1obeRqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 2. Design a better model that works directly on 28 x 28\n",
        " images"
      ],
      "metadata": {
        "id": "PfD-jTLkejCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplifiedSmallAlexnet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(128, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(128, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Mengurangi dimensi lebih awal\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Mengurangi dimensi lebih awal\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(256),  # Mengurangi jumlah neuron\n",
        "            nn.LazyLinear(num_classes)\n",
        "        )"
      ],
      "metadata": {
        "id": "dFBEbQrFeosr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Modify the batch size, and observe the changes in throughput (images/s), accuracy, and GPU memory."
      ],
      "metadata": {
        "id": "ng6dX14wetoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memodifikasi ukuran batch dapat memberikan dampak yang signifikan terhadap throughput (jumlah gambar yang diproses per detik), akurasi, dan penggunaan memori GPU. Namun, ukuran batch yang terlalu besar dapat menyebabkan penggunaan memori GPU yang lebih tinggi, berpotensi menyebabkan out-of-memory error, terutama pada arsitektur jaringan yang lebih dalam seperti AlexNet.\n",
        "\n",
        "Sebaliknya, mengurangi ukuran batch dapat menurunkan throughput tetapi mungkin meningkatkan akurasi, terutama jika ukuran batch yang kecil memungkinkan model untuk belajar dari variasi data yang lebih baik dan mengurangi risiko overfitting. P\n",
        "\n",
        "engaruh ukuran batch juga tergantung pada arsitektur model dan karakteristik dataset yang digunakan; oleh karena itu, eksperimen lebih lanjut diperlukan untuk menemukan ukuran batch yang optimal yang seimbang antara throughput dan akurasi sambil mempertimbangkan keterbatasan memori GPU."
      ],
      "metadata": {
        "id": "mZ2COGVceweV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Apply dropout and ReLU to LeNet-5. Does it improve? Can you improve things further by preprocessing to take advantage of the invariances inherent in the images?"
      ],
      "metadata": {
        "id": "Mt_SNr_Ye_TA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(nn.LazyConv2d(6, kernel_size=5, padding=2),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                 nn.LazyConv2d(16, kernel_size=5),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                 nn.Flatten(),\n",
        "                                 nn.LazyLinear(120),\n",
        "                                 nn.ReLU(), nn.Dropout(0.5),\n",
        "                                 nn.LazyLinear(84),\n",
        "                                 nn.ReLU(), nn.Dropout(0.5),\n",
        "                                 nn.LazyLinear(num_classes))\n",
        "model = LeNet(lr=0.01)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(28, 28))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "X,y = next(iter(data.get_dataloader(False)))\n",
        "X = X.to('cuda')\n",
        "y = y.to('cuda')\n",
        "y_hat = model(X)\n",
        "print(f'acc: {model.accuracy(y_hat,y).item():.2f}')"
      ],
      "metadata": {
        "id": "rfDlhS_WfDpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Can you make AlexNet overfit? Which feature do you need to remove or change to break training?"
      ],
      "metadata": {
        "id": "oAYu-iBVfISe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Mengurangi ukuran dataset pelatihan.\n",
        "* Meningkatkan kompleksitas model dengan menambah lapisan dan parameter.\n",
        "* Menghilangkan teknik regulasi.\n",
        "* Menggunakan tingkat training yang rendah dan jumlah epoch yang terbatas.\n",
        "* Memberikan label yang bersih tanpa noise."
      ],
      "metadata": {
        "id": "ip-iwyJEfMTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2. Networks Using Blocks (VGG)"
      ],
      "metadata": {
        "id": "5ot9Z-EcfYg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2.1. VGG Blocks"
      ],
      "metadata": {
        "id": "jUtU1otnfeek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg_block(num_convs, out_channels):\n",
        "    layers = []\n",
        "    for _ in range(num_convs):\n",
        "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "JAzN4ts0fYDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2.2. VGG Network"
      ],
      "metadata": {
        "id": "deT4K9SUfiFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(d2l.Classifier):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        conv_blks = []\n",
        "        for (num_convs, out_channels) in arch:\n",
        "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
        "        self.net = nn.Sequential(\n",
        "            *conv_blks, nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "xjEaquDNfoos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n",
        "    (1, 1, 224, 224))"
      ],
      "metadata": {
        "id": "6jvEmi00fpuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2.3. Training"
      ],
      "metadata": {
        "id": "B0NLT8LtfrMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "NCL8Qh1hfv44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2.5. Exercises"
      ],
      "metadata": {
        "id": "zt2JUqMcfyhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs more GPU memory."
      ],
      "metadata": {
        "id": "c52yOBYrgAs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1. Compare the number of parameters needed for AlexNet and VGG."
      ],
      "metadata": {
        "id": "geaizbqzgDVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
        "vgg = VGG(arch=arch)\n",
        "X = torch.randn(1,3, 224, 224)\n",
        "_ = vgg(X)\n",
        "params = {'conv':0, 'lr':0}\n",
        "for idx, module in enumerate(vgg.net):\n",
        "    if type(module) == nn.Sequential:\n",
        "        stat_params(module,params)\n",
        "    if type(module) == nn.Linear:\n",
        "        num = sum(p.numel() for p in module.parameters())\n",
        "        params['lr'] += num\n",
        "params"
      ],
      "metadata": {
        "id": "sOl2yn3VgG38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 2. Compare the number of floating point operations used in the convolutional layers and in the fully connected layers."
      ],
      "metadata": {
        "id": "cnTTxgYfgHak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1,3, 224, 224)\n",
        "params = {'conv':0, 'lr':0}\n",
        "for idx, module in enumerate(vgg.net):\n",
        "    if type(module) == nn.Sequential:\n",
        "        x = stat_comp(module, params, x)\n",
        "    if type(module) == nn.Linear:\n",
        "        params['lr'] += sum(p.numel() for p in module.parameters())\n",
        "params"
      ],
      "metadata": {
        "id": "vrSLGZ66gMlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(1,3, 224, 224)\n",
        "_ = vgg(X)\n",
        "total_params = sum(p.numel() for p in vgg.parameters())\n",
        "print(\"Total parameters:\", total_params)"
      ],
      "metadata": {
        "id": "hL_iyA4zgVI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 3. How could you reduce the computational cost created by the fully connected layers?"
      ],
      "metadata": {
        "id": "7u6CFwzIgNhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Global Average Pooling (GAP), lapisan konvolusional 1x1, pruning jaringan, aproksimasi Low-Rank, atau kuantisasi."
      ],
      "metadata": {
        "id": "wUyxBgdVgYlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. When displaying the dimensions associated with the various layers of the network, we only see the information associated with eight blocks (plus some auxiliary transforms), even though the network has 11 layers. Where did the remaining three layers go?"
      ],
      "metadata": {
        "id": "gYBuWhuigt2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg"
      ],
      "metadata": {
        "id": "jNFWYpuXg9EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blok konvolusi dianggap sebagai satu lapisan dalam jaringan, dan tiga blok konvolusi terakhir dalam VGG masing-masing terdiri dari dua lapisan konvolusi, sehingga menjadi tiga lapisan yang tersisa"
      ],
      "metadata": {
        "id": "LFn2d5FhgzeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Use Table 1 in the VGG paper (Simonyan and Zisserman, 2014) to construct other common models, such as VGG-16 or VGG-19.|"
      ],
      "metadata": {
        "id": "sDXJiTJXg3xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arch16=((2, 64), (2, 128), (3, 256), (3, 512), (3, 512))\n",
        "vgg16 = VGG(arch=arch16)\n",
        "vgg16"
      ],
      "metadata": {
        "id": "-cipF685hIQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arch19=((2, 64), (2, 128), (4, 256), (4, 512), (4, 512))\n",
        "vgg19 = VGG(arch=arch19)\n",
        "vgg19"
      ],
      "metadata": {
        "id": "ELRyykAvhKKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Upsampling the resolution in Fashion-MNIST eight-fold from 28x28 to 224 x 224 dimensions is very wasteful. Try modifying the network architecture and resolution conversion, e.g., to 56 or to 84 dimensions for its input instead. Can you do so without reducing the accuracy of the network? Consult the VGG paper (Simonyan and Zisserman, 2014) for ideas on adding more nonlinearities prior to downsampling."
      ],
      "metadata": {
        "id": "y3nL5bJNhNHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG(arch=((3, 128), (3, 256)), lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(28, 28))\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "f0D7BtZGhPQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.3. Network in Network (NiN)"
      ],
      "metadata": {
        "id": "Cx9zytdMhQQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3.1. NiN Blocks"
      ],
      "metadata": {
        "id": "7yqFKikjhZyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nin_block(out_channels, kernel_size, strides, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())"
      ],
      "metadata": {
        "id": "S36ecsNSheH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3.2. NiN Model"
      ],
      "metadata": {
        "id": "PcoodJfDhe8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NiN(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nin_block(96, kernel_size=11, strides=4, padding=0),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nin_block(256, kernel_size=5, strides=1, padding=2),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nin_block(384, kernel_size=3, strides=1, padding=1),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.Dropout(0.5),\n",
        "            nin_block(num_classes, kernel_size=3, strides=1, padding=1),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten())\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "25SEEWQ0hjVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NiN().layer_summary((1, 1, 224, 224))"
      ],
      "metadata": {
        "id": "4dyZTqCZhkAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3.3. Training"
      ],
      "metadata": {
        "id": "38BXLeiohmwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NiN(lr=0.05)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "E2kp7UCuhqO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3.5. Exercises"
      ],
      "metadata": {
        "id": "O0MskMRWhrGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why are there two 1x1 convolutional layers per NiN block? Increase their number to three. Reduce their number to one. What changes?"
      ],
      "metadata": {
        "id": "D9_Z1xwgh26E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dalam arsitektur Network in Network (NiN), lapisan konvolusi 1x1 digunakan untuk memperkenalkan non-linearitas tambahan dan meningkatkan kapasitas jaringan tanpa menambah terlalu banyak parameter. Penggunaan konvolusi 1x1 ini memiliki efek spesifik pada ekspresivitas dan kompleksitas jaringan:\n",
        "\n",
        "* Dua Lapisan Konvolusi 1x1 per Blok NiN:memungkinkan banyak jalur untuk transformasi fitur.\n",
        "\n",
        "* Tiga Lapisan Konvolusi 1x1 per Blok NiN: menambah kapasitas jaringan lebih jauh.\n",
        "\n",
        "* Satu Lapisan Konvolusi 1x1 per Blok NiN: mengurangi kompleksitas setiap blok NiN, membatasi kapasitas jaringan untuk menangkap interaksi fitur kompleks."
      ],
      "metadata": {
        "id": "9Y5To8Uvh7U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "arch = ((96,11,4,0,[[1,0]]),(256,5,1,2,[[1,0]]),(384,3,1,1,[[1,0]]),(10,3,1,1,[[1,0]]))\n",
        "model = Nin(arch, lr=0.05)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "X,y = next(iter(data.get_dataloader(False)))\n",
        "X = X.to('cuda')\n",
        "y = y.to('cuda')\n",
        "y_hat = model(X)\n",
        "print(f'acc: {model.accuracy(y_hat,y).item():.2f}')"
      ],
      "metadata": {
        "id": "xr0n51wHiYiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What changes if you replace the 1x1 convolutions by 3x3 convolutions?"
      ],
      "metadata": {
        "id": "1V5vpEqfiZME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arch = ((96,11,4,0,[[3,1],[3,1]]),(256,5,1,2,[[3,1],[3,1]]),(384,3,1,1,[[3,1],[3,1]]),(10,3,1,1,[[3,1],[3,1]]))\n",
        "model = Nin(arch)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "X,y = next(iter(data.get_dataloader(False)))\n",
        "X = X.to('cuda')\n",
        "y = y.to('cuda')\n",
        "y_hat = model(X)\n",
        "print(f'acc: {model.accuracy(y_hat,y).item():.2f}')"
      ],
      "metadata": {
        "id": "SEz7Zy1aic8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What happens if you replace the global average pooling by a fully connected layer (speed, accuracy, number of parameters)?"
      ],
      "metadata": {
        "id": "jHGT54plifrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPNin(d2l.Classifier):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        layers = []\n",
        "        for i in range(len(arch)-1):\n",
        "            layers.append(nin_block(*arch[i]))\n",
        "            layers.append(nn.MaxPool2d(3, stride=2))\n",
        "        layers.append(nn.Dropout(0.5))\n",
        "        layers.append(nin_block(*arch[-1]))\n",
        "        layers.append(nn.Flatten())\n",
        "        layers.append(nn.LazyLinear(num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "RlOvv3AXijy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Calculate the resource usage for NiN."
      ],
      "metadata": {
        "id": "qlhqwlSPimh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1. What is the number of parameters?"
      ],
      "metadata": {
        "id": "JTf6r7aNiqCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define a NiN block\n",
        "def nin_block(out_channels, kernel_size, strides, padding, conv1s=[(1, 0), (1, 0)]):\n",
        "    layers = [nn.LazyConv2d(out_channels, kernel_size=kernel_size, stride=strides, padding=padding), nn.ReLU()]\n",
        "    for conv1_size, conv1_padding in conv1s:\n",
        "        layers.append(nn.LazyConv2d(out_channels, kernel_size=conv1_size, padding=conv1_padding))\n",
        "        layers.append(nn.ReLU())\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Define the NiN architecture using nin_block\n",
        "class Nin(nn.Module):\n",
        "    def __init__(self, arch):\n",
        "        super(Nin, self).__init__()\n",
        "        self.net = nn.Sequential()\n",
        "        for i, (out_channels, kernel_size, strides, padding, num_conv1) in enumerate(arch):\n",
        "            conv1s = [(1, 0) for _ in range(num_conv1)]  # Creating a list of (kernel_size, padding) for conv1\n",
        "            self.net.add_module(f\"block_{i}\", nin_block(out_channels, kernel_size, strides, padding, conv1s))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Model definition and summary\n",
        "arch = ((96, 11, 4, 0, 2), (256, 5, 1, 2, 2), (384, 3, 1, 1, 2), (10, 3, 1, 1, 2))\n",
        "model = Nin(arch)\n",
        "\n",
        "# Dummy input and parameter calculation\n",
        "X = torch.randn(1, 3, 224, 224)\n",
        "_ = model(X)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)"
      ],
      "metadata": {
        "id": "fQCLmoAbiv1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 2. What is the amount of computation?"
      ],
      "metadata": {
        "id": "qPoaeKbeiwaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required package\n",
        "!pip install thop\n",
        "\n",
        "# Import the profile function from thop and calculate FLOPs and parameters\n",
        "from thop import profile\n",
        "\n",
        "# Calculate FLOPs and parameters for the model\n",
        "flops, params = profile(model, inputs=(X,))\n",
        "print(\"Total FLOPs:\", flops)\n",
        "print(\"Total parameters:\", params)"
      ],
      "metadata": {
        "id": "YAPOOZFWi1xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 3. What is the amount of memory needed during training?"
      ],
      "metadata": {
        "id": "Cu-csXGgi334"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from d2l import torch as d2l\n",
        "\n",
        "class Nin(d2l.Classifier):\n",
        "    def __init__(self, arch, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential()\n",
        "        for i, (out_channels, kernel_size, strides, padding, num_conv1) in enumerate(arch):\n",
        "            conv1s = [(1, 0) for _ in range(num_conv1)]  # Creating a list of (kernel_size, padding) for conv1\n",
        "            self.net.add_module(f\"block_{i}\", nin_block(out_channels, kernel_size, strides, padding, conv1s))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "IAqbHr1Ni8Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 4. What is the amount of memory needed during prediction?"
      ],
      "metadata": {
        "id": "BzXtyswEi8v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.empty_cache()\n",
        "_ = model(X)\n",
        "memory_stats = torch.cuda.memory_stats(device=device)\n",
        "print(\"Peak memory usage:\", memory_stats[\"allocated_bytes.all.peak\"] / (1024 ** 2), \"MB\")\n",
        "print(\"Current memory usage:\", memory_stats[\"allocated_bytes.all.current\"] / (1024 ** 2), \"MB\")"
      ],
      "metadata": {
        "id": "lzA3Bt8ejBni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are possible problems with reducing the 384 x 5 x 5 representation to a 10 x 5 x 5 representation in one step?"
      ],
      "metadata": {
        "id": "4kUrLvLljFkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Kehilangan Informasi\n",
        "* Underfitting\n",
        "* Penyempitan Informasi\n",
        "* Pengurangan Ekspresivitas\n",
        "* Fitur Spasial\n",
        "* Kehilangan Daya Diskriminatif"
      ],
      "metadata": {
        "id": "t8RV1AxGjKTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Use the structural design decisions in VGG that led to VGG-11, VGG-16, and VGG-19 to design a family of NiN-like networks."
      ],
      "metadata": {
        "id": "C_5QV0uLjW-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arch = ((96,11,4,0,2),(256,5,1,2,2),(384,3,1,1,2),(10,3,1,1,2))\n",
        "nin = Nin(arch)\n",
        "nin"
      ],
      "metadata": {
        "id": "sCCWuCbDjayc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Nin(d2l.Classifier):\n",
        "    def __init__(self, arch, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential()\n",
        "        for i, (out_channels, kernel_size, strides, padding) in enumerate(arch):\n",
        "            # Only include a basic convolutional layer without additional 1x1 convolutions\n",
        "            self.net.add_module(f\"block_{i}\", nin_block(out_channels, kernel_size, strides, padding))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def nin_block(out_channels, kernel_size, strides, padding):\n",
        "    layers = [\n",
        "        nn.LazyConv2d(out_channels, kernel_size=kernel_size, stride=strides, padding=padding),\n",
        "        nn.ReLU()\n",
        "    ]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "arch15 = ((64, 3, 2, 1),\n",
        "          (256, 3, 1, 1),\n",
        "          (256, 3, 1, 1),\n",
        "          (384, 3, 1, 1),\n",
        "          (10, 3, 1, 1))\n",
        "\n",
        "nin15 = Nin(arch15)\n",
        "print(nin15)"
      ],
      "metadata": {
        "id": "KcorEHl-jc3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.4. Multi-Branch Networks (GoogLeNet)"
      ],
      "metadata": {
        "id": "sxLRnzFXjhC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4.1. Inception Blocks"
      ],
      "metadata": {
        "id": "Z2clPMLOjtG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Inception(nn.Module):\n",
        "    # c1--c4 are the number of output channels for each branch\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super(Inception, self).__init__(**kwargs)\n",
        "        # Branch 1\n",
        "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
        "        # Branch 2\n",
        "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
        "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
        "        # Branch 3\n",
        "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
        "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
        "        # Branch 4\n",
        "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = F.relu(self.b1_1(x))\n",
        "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
        "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
        "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
        "        return torch.cat((b1, b2, b3, b4), dim=1)"
      ],
      "metadata": {
        "id": "pPehKAPEjpha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4.2. GoogLeNet Model"
      ],
      "metadata": {
        "id": "i1s3tx0mjwv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogleNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "WSO6If2oj0PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b2(self):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),\n",
        "        nn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "1ep57APxj05r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b3(self):\n",
        "    return nn.Sequential(Inception(64, (96, 128), (16, 32), 32),\n",
        "                         Inception(128, (128, 192), (32, 96), 64),\n",
        "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "0mFwVPWqj3U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b4(self):\n",
        "    return nn.Sequential(Inception(192, (96, 208), (16, 48), 64),\n",
        "                         Inception(160, (112, 224), (24, 64), 64),\n",
        "                         Inception(128, (128, 256), (24, 64), 64),\n",
        "                         Inception(112, (144, 288), (32, 64), 64),\n",
        "                         Inception(256, (160, 320), (32, 128), 128),\n",
        "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "tZ6JTQHyj30B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b5(self):\n",
        "    return nn.Sequential(Inception(256, (160, 320), (32, 128), 128),\n",
        "                         Inception(384, (192, 384), (48, 128), 128),\n",
        "                         nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())"
      ],
      "metadata": {
        "id": "2biHiqQOj6ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b5(self):\n",
        "    return nn.Sequential(Inception(256, (160, 320), (32, 128), 128),\n",
        "                         Inception(384, (192, 384), (48, 128), 128),\n",
        "                         nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())"
      ],
      "metadata": {
        "id": "_wX2RbT_j6kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogleNet().layer_summary((1, 1, 96, 96))"
      ],
      "metadata": {
        "id": "cfeOqWYhj9cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4.3. Training"
      ],
      "metadata": {
        "id": "cWIpJdEAj_O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogleNet(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "GK4MltSNkBGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.5. Batch Normalization"
      ],
      "metadata": {
        "id": "Ek_54922kKJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5.3. Implementation from Scratch"
      ],
      "metadata": {
        "id": "5Dl4Bi3zkRx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # Use is_grad_enabled to determine whether we are in training mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of X, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # In training mode, the current mean and variance are used\n",
        "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
        "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_mean.data, moving_var.data"
      ],
      "metadata": {
        "id": "RzihE8fFkRCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm(nn.Module):\n",
        "    # num_features: the number of outputs for a fully connected layer or the\n",
        "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
        "    # fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and\n",
        "        # 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
        "        # the device where X is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated moving_mean and moving_var\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.1)\n",
        "        return Y"
      ],
      "metadata": {
        "id": "3ktWs4-mkWK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5.4. LeNet with Batch Normalization"
      ],
      "metadata": {
        "id": "zwjh153okZfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BNLeNetScratch(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120),\n",
        "            BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),\n",
        "            BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))"
      ],
      "metadata": {
        "id": "CbJSG_qDkbh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128)\n",
        "model = BNLeNetScratch(lr=0.1)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "0kFNWBFikdQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.net[1].gamma.reshape((-1,)), model.net[1].beta.reshape((-1,))"
      ],
      "metadata": {
        "id": "L0Log_fokeyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5.5. Concise Implementation"
      ],
      "metadata": {
        "id": "kDkx79SZkhOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BNLeNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
        "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
        "            nn.Sigmoid(), nn.LazyLinear(num_classes))"
      ],
      "metadata": {
        "id": "VSk6Ilickj9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128)\n",
        "model = BNLeNet(lr=0.1)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "bH4am4kPkomm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.6. Residual Networks (ResNet) and ResNeXt"
      ],
      "metadata": {
        "id": "Z-VO0fFKkqdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.6.2. Residual Blocks"
      ],
      "metadata": {
        "id": "nEUH5sGdkzEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
        "                                   stride=strides)\n",
        "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ],
      "metadata": {
        "id": "iALu808ik1Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(3)\n",
        "X = torch.randn(4, 3, 6, 6)\n",
        "blk(X).shape"
      ],
      "metadata": {
        "id": "rWant-f0k5-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(6, use_1x1conv=True, strides=2)\n",
        "blk(X).shape"
      ],
      "metadata": {
        "id": "_PPD1bmXk7fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.6.3. ResNet Model"
      ],
      "metadata": {
        "id": "ixA3_kx7lANu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "pUbitFOYlC3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(ResNet)\n",
        "def block(self, num_residuals, num_channels, first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels))\n",
        "    return nn.Sequential(*blk)"
      ],
      "metadata": {
        "id": "Ho2dIr-glDeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(ResNet)\n",
        "def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1())\n",
        "    for i, b in enumerate(arch):\n",
        "        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
        "    self.net.add_module('last', nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "gmOmCzA2lE_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(ResNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
        "                       lr, num_classes)\n",
        "\n",
        "ResNet18().layer_summary((1, 1, 96, 96))"
      ],
      "metadata": {
        "id": "Szv2eAkmlG_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.6.4. Training"
      ],
      "metadata": {
        "id": "JdQALjwQlNZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "EOnFuLu0lOYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.6.5. ResNeXt"
      ],
      "metadata": {
        "id": "nAtrh2CalRA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNeXtBlock(nn.Module):\n",
        "    \"\"\"The ResNeXt block.\"\"\"\n",
        "    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n",
        "                 strides=1):\n",
        "        super().__init__()\n",
        "        bot_channels = int(round(num_channels * bot_mul))\n",
        "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n",
        "                                   stride=strides, padding=1,\n",
        "                                   groups=bot_channels//groups)\n",
        "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "        self.bn3 = nn.LazyBatchNorm2d()\n",
        "        if use_1x1conv:\n",
        "            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "            self.bn4 = nn.LazyBatchNorm2d()\n",
        "        else:\n",
        "            self.conv4 = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "        if self.conv4:\n",
        "            X = self.bn4(self.conv4(X))\n",
        "        return F.relu(Y + X)"
      ],
      "metadata": {
        "id": "m29Sk1AslU73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = ResNeXtBlock(32, 16, 1)\n",
        "X = torch.randn(4, 32, 96, 96)\n",
        "blk(X).shape"
      ],
      "metadata": {
        "id": "AbvWT3f-lXZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJMmGVO6ELXn"
      },
      "source": [
        "# 8.7. Densely Connected Networks (DenseNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suzz76KyFiZd"
      },
      "source": [
        "## 8.7.2. Dense Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8SsNhBFEJ0m"
      },
      "outputs": [],
      "source": [
        "def conv_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KBju3DrFkDq"
      },
      "outputs": [],
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, num_convs, num_channels):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layer = []\n",
        "        for i in range(num_convs):\n",
        "            layer.append(conv_block(num_channels))\n",
        "        self.net = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for blk in self.net:\n",
        "            Y = blk(X)\n",
        "            # Concatenate input and output of each block along the channels\n",
        "            X = torch.cat((X, Y), dim=1)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9eNZZ-UFlY8",
        "outputId": "006e12ff-ce82-46d8-f4ff-b60966ef9bb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 23, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "blk = DenseBlock(2, 10)\n",
        "X = torch.randn(4, 3, 8, 8)\n",
        "Y = blk(X)\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8ekYGWxFpOU"
      },
      "source": [
        "## 8.7.3. Transition Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C7-HOVdFm-S"
      },
      "outputs": [],
      "source": [
        "def transition_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab4g-WGNFrFE",
        "outputId": "bcd015e2-cf15-41b6-c539-c61e696d9f08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 10, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "blk = transition_block(10)\n",
        "blk(Y).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4muf-_NFuaE"
      },
      "source": [
        "## 8.7.4. DenseNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3UnemXpFsmM"
      },
      "outputs": [],
      "source": [
        "class DenseNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78BROEe_FyD3"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(DenseNet)\n",
        "def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),\n",
        "             lr=0.1, num_classes=10):\n",
        "    super(DenseNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1())\n",
        "    for i, num_convs in enumerate(arch):\n",
        "        self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,\n",
        "                                                          growth_rate))\n",
        "        # The number of output channels in the previous dense block\n",
        "        num_channels += num_convs * growth_rate\n",
        "        # A transition layer that halves the number of channels is added\n",
        "        # between the dense blocks\n",
        "        if i != len(arch) - 1:\n",
        "            num_channels //= 2\n",
        "            self.net.add_module(f'tran_blk{i+1}', transition_block(\n",
        "                num_channels))\n",
        "    self.net.add_module('last', nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ec-apeF19p"
      },
      "source": [
        "## 8.7.5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__WEMvQOF0Qv"
      },
      "outputs": [],
      "source": [
        "model = DenseNet(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3GMvrrKHEuT"
      },
      "source": [
        "## 8.7.7. Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGl5NVwcHG49"
      },
      "source": [
        "1. Why do we use average pooling rather than max-pooling in the transition layer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u96lSfIIKnC2"
      },
      "source": [
        "*  ***Feature Map* yang lebih halus**:\n",
        "\n",
        "*Average pooling* menghitung nilai rata-rata dari\n",
        "elemen-elemen di *pooling window*, sehingga menghasilkan *feature map* yang lebih halus dan umum, juga kurang sensitif terhadap noise atau pola spesifik dibandingkan *max pooling*. Pada *transition layer*, hal ini membantu mencegah model dari overfitting pada detail kecil di *layer* awal dan lebih fokus pada struktur keseluruhan input.\n",
        "\n",
        "*  **Mempertahankan informasi dengan lebih baik**:\n",
        "\n",
        "*Max pooling* hanya menyimpan fitur yang paling dominan di setiap area, yang dapat menyebabkan hilangnya informasi penting. Sedangkan, *Average pooling* menangkap distribusi fitur yang lebih luas, yang bisa menghasilkan representasi yang lebih baik.\n",
        "\n",
        "*  ***Gradient Flow***:\n",
        "\n",
        "Dalam beberapa arsitektur, seperti DenseNet, *average pooling* lebih sering dipakai di *transition layer* karena memungkinkan *gradient flow* yang lebih baik selama *backpropagation*. Sifat *average pooling* yang lebih seimbang (dibandingkan *max pooling*) memastikan *gradien* cenderung tidak menghilang, sehingga pelatihan menjadi lebih stabil.\n",
        "\n",
        "*  **Reduksi Dimensi**:\n",
        "\n",
        "Baik *max pooling* maupun *average pooling*, keduanya dimensi spasial dari *feature map*, tetapi *average pooling* melakukannya dengan cara lebih menjaga pola-pola halus dalam gambar, yang dapat berguna dalam mempertahankan integritas data saat melewati *transition layer*."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. One of the advantages mentioned in the DenseNet paper is that its model parameters are smaller than those of ResNet. Why is this the case?"
      ],
      "metadata": {
        "id": "Uct4wAS-2l3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet memiliki parameter yang lebih kecil daripada ResNet karena pemakaian ulang fitur. Setiap *layer* DenseNet menerima input dari semua *layer* sebelumnya, sehingga fitur tidak perlu dipelajari ulang di setiap *layer*. Selain itu, DenseNet menggunakan *convolutional layer* yang lebih kecil, tanpa *bottleneck* yang berat seperti di ResNet, dan konektivitas antar lapisan yang lebih efisien, sehingga mengurangi jumlah parameter secara keseluruhan."
      ],
      "metadata": {
        "id": "lLTQGAcT3ztq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. One problem for which DenseNet has been criticized is its high memory consumption.\n",
        "\n",
        " 1.   Is this really the case? Try to change the input shape to\n",
        " to compare the actual GPU memory consumption empirically.\n",
        "\n",
        " 2.   Can you think of an alternative means of reducing the memory consumption? How would you need to change the framework?"
      ],
      "metadata": {
        "id": "ifQSuKgP4im7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet memiliki konsumsi memori yang tinggi karena setiap *layer* terhubung dengan semua *layer* sebelumnya, yang menyebabkan setiap *layer* harus menyimpan banyak fitur dari semua *layer* sebelumnya."
      ],
      "metadata": {
        "id": "RufvBBy_6F4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk membandingkan penggunaan memori GPU antara DenseNet dan model lain (misalnya, ResNet), kita dapat mengubah input shape dan mengukur penggunaan memori GPU. Berikut langkah-langkah umum untuk melakukan eksperimen ini:\n",
        "\n",
        "1.  Jalankan DenseNet dan ResNet dengan input yang sama, misalnya input dengan ukuran 224x224.\n",
        "2.  Ukur penggunaan memori GPU menggunakan alat seperti nvidia-smi atau melalui framework seperti PyTorch yang memungkinkan kita untuk melacak penggunaan memori GPU.\n",
        "3.  Ubah ukuran input (misalnya, dari 224x224 menjadi 512x512) dan ulangi pengukuran.\n",
        "4.  Bandingkan penggunaan memori GPU untuk setiap ukuran input pada DenseNet dan ResNet untuk melihat perbedaannya."
      ],
      "metadata": {
        "id": "zApVO5dk_22f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beberapa alternatif untuk mengurangi konsumsi memori DenseNet adalah:\n",
        "\n",
        "*  Gradient Checkpointing\n",
        "*  Mengurangi Growth Rate\n",
        "*  Pruning\n",
        "*  Menggunakan 1x1 Convolutions\n",
        "\n",
        "Untuk mengimplementasikan solusi-solusi ini, *framework* Deep Learning seperti PyTorch atau TensorFlow perlu mendukung beberapa fitur:\n",
        "\n",
        "Gradient Checkpointing: Framework ini sudah mendukung teknik gradient checkpointing secara native, jadi pengguna hanya perlu mengaktifkan fitur ini.\n",
        "\n",
        "Pruning: Framework perlu mendukung algoritma pruning atau menyediakan API yang memungkinkan pengurangan koneksi antar-lapisan.\n",
        "\n",
        "Dynamic Growth Rate: Framework bisa mendukung pengaturan growth rate yang lebih adaptif, memungkinkan penyesuaian growth rate secara otomatis berdasarkan kebutuhan memori atau komputasi."
      ],
      "metadata": {
        "id": "yAmLPZmlAw6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Implement the various DenseNet versions presented in Table 1 of the DenseNet paper (Huang et al., 2017)."
      ],
      "metadata": {
        "id": "XcW1ydR1BKXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "eziplE2KEx82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(num_channels, kernel_size, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=kernel_size, padding=padding))\n",
        "\n",
        "def transition_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, convs, num_channels):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layer = []\n",
        "        for kernel_size, padding in convs:\n",
        "            layer.append(conv_block(num_channels, kernel_size, padding))\n",
        "        self.net = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for blk in self.net:\n",
        "            Y = blk(X)\n",
        "            # Concatenate input and output of each block along the channels\n",
        "            X = torch.cat((X, Y), dim=1)\n",
        "        return X\n",
        "\n",
        "class DenseNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def __init__(self, num_channels=64, growth_rate=32, arch=[[[3,1],[3,1]],[[3,1],[3,1]]],lr=0.1, num_classes=10):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(self.b1())\n",
        "        for i, convs in enumerate(arch):\n",
        "            self.net.add_module(f'dense_blk{i+1}', DenseBlock(convs, growth_rate))\n",
        "            # The number of output channels in the previous dense block\n",
        "            num_channels += len(convs) * growth_rate\n",
        "            # A transition layer that halves the number of channels is added\n",
        "            # between the dense blocks\n",
        "            if i != len(arch) - 1:\n",
        "                num_channels //= 2\n",
        "                self.net.add_module(f'tran_blk{i+1}', transition_block(\n",
        "                    num_channels))\n",
        "        self.net.add_module('last', nn.Sequential(\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "twf4iG0LEFp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = d2l.FashionMNIST(batch_size=32, resize=(224, 224))\n",
        "arch121 = ([[[1,0],[3,1]]*6,[[1,0],[3,1]]*12,[[1,0],[3,1]]*24,[[1,0],[3,1]]*16])\n",
        "densenet121 = DenseNet(lr=0.01, arch=arch121)\n",
        "densenet121.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "# print(count_parameters(model))\n",
        "summary(densenet121, (1, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGGTe9ziEV3T",
        "outputId": "a923f73c-2975-42ed-e99c-8a624f8ffbac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           3,200\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "              ReLU-6           [-1, 64, 56, 56]               0\n",
            "            Conv2d-7           [-1, 32, 56, 56]           2,080\n",
            "       BatchNorm2d-8           [-1, 96, 56, 56]             192\n",
            "              ReLU-9           [-1, 96, 56, 56]               0\n",
            "           Conv2d-10           [-1, 32, 56, 56]          27,680\n",
            "      BatchNorm2d-11          [-1, 128, 56, 56]             256\n",
            "             ReLU-12          [-1, 128, 56, 56]               0\n",
            "           Conv2d-13           [-1, 32, 56, 56]           4,128\n",
            "      BatchNorm2d-14          [-1, 160, 56, 56]             320\n",
            "             ReLU-15          [-1, 160, 56, 56]               0\n",
            "           Conv2d-16           [-1, 32, 56, 56]          46,112\n",
            "      BatchNorm2d-17          [-1, 192, 56, 56]             384\n",
            "             ReLU-18          [-1, 192, 56, 56]               0\n",
            "           Conv2d-19           [-1, 32, 56, 56]           6,176\n",
            "      BatchNorm2d-20          [-1, 224, 56, 56]             448\n",
            "             ReLU-21          [-1, 224, 56, 56]               0\n",
            "           Conv2d-22           [-1, 32, 56, 56]          64,544\n",
            "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
            "             ReLU-24          [-1, 256, 56, 56]               0\n",
            "           Conv2d-25           [-1, 32, 56, 56]           8,224\n",
            "      BatchNorm2d-26          [-1, 288, 56, 56]             576\n",
            "             ReLU-27          [-1, 288, 56, 56]               0\n",
            "           Conv2d-28           [-1, 32, 56, 56]          82,976\n",
            "      BatchNorm2d-29          [-1, 320, 56, 56]             640\n",
            "             ReLU-30          [-1, 320, 56, 56]               0\n",
            "           Conv2d-31           [-1, 32, 56, 56]          10,272\n",
            "      BatchNorm2d-32          [-1, 352, 56, 56]             704\n",
            "             ReLU-33          [-1, 352, 56, 56]               0\n",
            "           Conv2d-34           [-1, 32, 56, 56]         101,408\n",
            "      BatchNorm2d-35          [-1, 384, 56, 56]             768\n",
            "             ReLU-36          [-1, 384, 56, 56]               0\n",
            "           Conv2d-37           [-1, 32, 56, 56]          12,320\n",
            "      BatchNorm2d-38          [-1, 416, 56, 56]             832\n",
            "             ReLU-39          [-1, 416, 56, 56]               0\n",
            "           Conv2d-40           [-1, 32, 56, 56]         119,840\n",
            "       DenseBlock-41          [-1, 448, 56, 56]               0\n",
            "      BatchNorm2d-42          [-1, 448, 56, 56]             896\n",
            "             ReLU-43          [-1, 448, 56, 56]               0\n",
            "           Conv2d-44          [-1, 224, 56, 56]         100,576\n",
            "        AvgPool2d-45          [-1, 224, 28, 28]               0\n",
            "      BatchNorm2d-46          [-1, 224, 28, 28]             448\n",
            "             ReLU-47          [-1, 224, 28, 28]               0\n",
            "           Conv2d-48           [-1, 32, 28, 28]           7,200\n",
            "      BatchNorm2d-49          [-1, 256, 28, 28]             512\n",
            "             ReLU-50          [-1, 256, 28, 28]               0\n",
            "           Conv2d-51           [-1, 32, 28, 28]          73,760\n",
            "      BatchNorm2d-52          [-1, 288, 28, 28]             576\n",
            "             ReLU-53          [-1, 288, 28, 28]               0\n",
            "           Conv2d-54           [-1, 32, 28, 28]           9,248\n",
            "      BatchNorm2d-55          [-1, 320, 28, 28]             640\n",
            "             ReLU-56          [-1, 320, 28, 28]               0\n",
            "           Conv2d-57           [-1, 32, 28, 28]          92,192\n",
            "      BatchNorm2d-58          [-1, 352, 28, 28]             704\n",
            "             ReLU-59          [-1, 352, 28, 28]               0\n",
            "           Conv2d-60           [-1, 32, 28, 28]          11,296\n",
            "      BatchNorm2d-61          [-1, 384, 28, 28]             768\n",
            "             ReLU-62          [-1, 384, 28, 28]               0\n",
            "           Conv2d-63           [-1, 32, 28, 28]         110,624\n",
            "      BatchNorm2d-64          [-1, 416, 28, 28]             832\n",
            "             ReLU-65          [-1, 416, 28, 28]               0\n",
            "           Conv2d-66           [-1, 32, 28, 28]          13,344\n",
            "      BatchNorm2d-67          [-1, 448, 28, 28]             896\n",
            "             ReLU-68          [-1, 448, 28, 28]               0\n",
            "           Conv2d-69           [-1, 32, 28, 28]         129,056\n",
            "      BatchNorm2d-70          [-1, 480, 28, 28]             960\n",
            "             ReLU-71          [-1, 480, 28, 28]               0\n",
            "           Conv2d-72           [-1, 32, 28, 28]          15,392\n",
            "      BatchNorm2d-73          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-74          [-1, 512, 28, 28]               0\n",
            "           Conv2d-75           [-1, 32, 28, 28]         147,488\n",
            "      BatchNorm2d-76          [-1, 544, 28, 28]           1,088\n",
            "             ReLU-77          [-1, 544, 28, 28]               0\n",
            "           Conv2d-78           [-1, 32, 28, 28]          17,440\n",
            "      BatchNorm2d-79          [-1, 576, 28, 28]           1,152\n",
            "             ReLU-80          [-1, 576, 28, 28]               0\n",
            "           Conv2d-81           [-1, 32, 28, 28]         165,920\n",
            "      BatchNorm2d-82          [-1, 608, 28, 28]           1,216\n",
            "             ReLU-83          [-1, 608, 28, 28]               0\n",
            "           Conv2d-84           [-1, 32, 28, 28]          19,488\n",
            "      BatchNorm2d-85          [-1, 640, 28, 28]           1,280\n",
            "             ReLU-86          [-1, 640, 28, 28]               0\n",
            "           Conv2d-87           [-1, 32, 28, 28]         184,352\n",
            "      BatchNorm2d-88          [-1, 672, 28, 28]           1,344\n",
            "             ReLU-89          [-1, 672, 28, 28]               0\n",
            "           Conv2d-90           [-1, 32, 28, 28]          21,536\n",
            "      BatchNorm2d-91          [-1, 704, 28, 28]           1,408\n",
            "             ReLU-92          [-1, 704, 28, 28]               0\n",
            "           Conv2d-93           [-1, 32, 28, 28]         202,784\n",
            "      BatchNorm2d-94          [-1, 736, 28, 28]           1,472\n",
            "             ReLU-95          [-1, 736, 28, 28]               0\n",
            "           Conv2d-96           [-1, 32, 28, 28]          23,584\n",
            "      BatchNorm2d-97          [-1, 768, 28, 28]           1,536\n",
            "             ReLU-98          [-1, 768, 28, 28]               0\n",
            "           Conv2d-99           [-1, 32, 28, 28]         221,216\n",
            "     BatchNorm2d-100          [-1, 800, 28, 28]           1,600\n",
            "            ReLU-101          [-1, 800, 28, 28]               0\n",
            "          Conv2d-102           [-1, 32, 28, 28]          25,632\n",
            "     BatchNorm2d-103          [-1, 832, 28, 28]           1,664\n",
            "            ReLU-104          [-1, 832, 28, 28]               0\n",
            "          Conv2d-105           [-1, 32, 28, 28]         239,648\n",
            "     BatchNorm2d-106          [-1, 864, 28, 28]           1,728\n",
            "            ReLU-107          [-1, 864, 28, 28]               0\n",
            "          Conv2d-108           [-1, 32, 28, 28]          27,680\n",
            "     BatchNorm2d-109          [-1, 896, 28, 28]           1,792\n",
            "            ReLU-110          [-1, 896, 28, 28]               0\n",
            "          Conv2d-111           [-1, 32, 28, 28]         258,080\n",
            "     BatchNorm2d-112          [-1, 928, 28, 28]           1,856\n",
            "            ReLU-113          [-1, 928, 28, 28]               0\n",
            "          Conv2d-114           [-1, 32, 28, 28]          29,728\n",
            "     BatchNorm2d-115          [-1, 960, 28, 28]           1,920\n",
            "            ReLU-116          [-1, 960, 28, 28]               0\n",
            "          Conv2d-117           [-1, 32, 28, 28]         276,512\n",
            "      DenseBlock-118          [-1, 992, 28, 28]               0\n",
            "     BatchNorm2d-119          [-1, 992, 28, 28]           1,984\n",
            "            ReLU-120          [-1, 992, 28, 28]               0\n",
            "          Conv2d-121          [-1, 496, 28, 28]         492,528\n",
            "       AvgPool2d-122          [-1, 496, 14, 14]               0\n",
            "     BatchNorm2d-123          [-1, 496, 14, 14]             992\n",
            "            ReLU-124          [-1, 496, 14, 14]               0\n",
            "          Conv2d-125           [-1, 32, 14, 14]          15,904\n",
            "     BatchNorm2d-126          [-1, 528, 14, 14]           1,056\n",
            "            ReLU-127          [-1, 528, 14, 14]               0\n",
            "          Conv2d-128           [-1, 32, 14, 14]         152,096\n",
            "     BatchNorm2d-129          [-1, 560, 14, 14]           1,120\n",
            "            ReLU-130          [-1, 560, 14, 14]               0\n",
            "          Conv2d-131           [-1, 32, 14, 14]          17,952\n",
            "     BatchNorm2d-132          [-1, 592, 14, 14]           1,184\n",
            "            ReLU-133          [-1, 592, 14, 14]               0\n",
            "          Conv2d-134           [-1, 32, 14, 14]         170,528\n",
            "     BatchNorm2d-135          [-1, 624, 14, 14]           1,248\n",
            "            ReLU-136          [-1, 624, 14, 14]               0\n",
            "          Conv2d-137           [-1, 32, 14, 14]          20,000\n",
            "     BatchNorm2d-138          [-1, 656, 14, 14]           1,312\n",
            "            ReLU-139          [-1, 656, 14, 14]               0\n",
            "          Conv2d-140           [-1, 32, 14, 14]         188,960\n",
            "     BatchNorm2d-141          [-1, 688, 14, 14]           1,376\n",
            "            ReLU-142          [-1, 688, 14, 14]               0\n",
            "          Conv2d-143           [-1, 32, 14, 14]          22,048\n",
            "     BatchNorm2d-144          [-1, 720, 14, 14]           1,440\n",
            "            ReLU-145          [-1, 720, 14, 14]               0\n",
            "          Conv2d-146           [-1, 32, 14, 14]         207,392\n",
            "     BatchNorm2d-147          [-1, 752, 14, 14]           1,504\n",
            "            ReLU-148          [-1, 752, 14, 14]               0\n",
            "          Conv2d-149           [-1, 32, 14, 14]          24,096\n",
            "     BatchNorm2d-150          [-1, 784, 14, 14]           1,568\n",
            "            ReLU-151          [-1, 784, 14, 14]               0\n",
            "          Conv2d-152           [-1, 32, 14, 14]         225,824\n",
            "     BatchNorm2d-153          [-1, 816, 14, 14]           1,632\n",
            "            ReLU-154          [-1, 816, 14, 14]               0\n",
            "          Conv2d-155           [-1, 32, 14, 14]          26,144\n",
            "     BatchNorm2d-156          [-1, 848, 14, 14]           1,696\n",
            "            ReLU-157          [-1, 848, 14, 14]               0\n",
            "          Conv2d-158           [-1, 32, 14, 14]         244,256\n",
            "     BatchNorm2d-159          [-1, 880, 14, 14]           1,760\n",
            "            ReLU-160          [-1, 880, 14, 14]               0\n",
            "          Conv2d-161           [-1, 32, 14, 14]          28,192\n",
            "     BatchNorm2d-162          [-1, 912, 14, 14]           1,824\n",
            "            ReLU-163          [-1, 912, 14, 14]               0\n",
            "          Conv2d-164           [-1, 32, 14, 14]         262,688\n",
            "     BatchNorm2d-165          [-1, 944, 14, 14]           1,888\n",
            "            ReLU-166          [-1, 944, 14, 14]               0\n",
            "          Conv2d-167           [-1, 32, 14, 14]          30,240\n",
            "     BatchNorm2d-168          [-1, 976, 14, 14]           1,952\n",
            "            ReLU-169          [-1, 976, 14, 14]               0\n",
            "          Conv2d-170           [-1, 32, 14, 14]         281,120\n",
            "     BatchNorm2d-171         [-1, 1008, 14, 14]           2,016\n",
            "            ReLU-172         [-1, 1008, 14, 14]               0\n",
            "          Conv2d-173           [-1, 32, 14, 14]          32,288\n",
            "     BatchNorm2d-174         [-1, 1040, 14, 14]           2,080\n",
            "            ReLU-175         [-1, 1040, 14, 14]               0\n",
            "          Conv2d-176           [-1, 32, 14, 14]         299,552\n",
            "     BatchNorm2d-177         [-1, 1072, 14, 14]           2,144\n",
            "            ReLU-178         [-1, 1072, 14, 14]               0\n",
            "          Conv2d-179           [-1, 32, 14, 14]          34,336\n",
            "     BatchNorm2d-180         [-1, 1104, 14, 14]           2,208\n",
            "            ReLU-181         [-1, 1104, 14, 14]               0\n",
            "          Conv2d-182           [-1, 32, 14, 14]         317,984\n",
            "     BatchNorm2d-183         [-1, 1136, 14, 14]           2,272\n",
            "            ReLU-184         [-1, 1136, 14, 14]               0\n",
            "          Conv2d-185           [-1, 32, 14, 14]          36,384\n",
            "     BatchNorm2d-186         [-1, 1168, 14, 14]           2,336\n",
            "            ReLU-187         [-1, 1168, 14, 14]               0\n",
            "          Conv2d-188           [-1, 32, 14, 14]         336,416\n",
            "     BatchNorm2d-189         [-1, 1200, 14, 14]           2,400\n",
            "            ReLU-190         [-1, 1200, 14, 14]               0\n",
            "          Conv2d-191           [-1, 32, 14, 14]          38,432\n",
            "     BatchNorm2d-192         [-1, 1232, 14, 14]           2,464\n",
            "            ReLU-193         [-1, 1232, 14, 14]               0\n",
            "          Conv2d-194           [-1, 32, 14, 14]         354,848\n",
            "     BatchNorm2d-195         [-1, 1264, 14, 14]           2,528\n",
            "            ReLU-196         [-1, 1264, 14, 14]               0\n",
            "          Conv2d-197           [-1, 32, 14, 14]          40,480\n",
            "     BatchNorm2d-198         [-1, 1296, 14, 14]           2,592\n",
            "            ReLU-199         [-1, 1296, 14, 14]               0\n",
            "          Conv2d-200           [-1, 32, 14, 14]         373,280\n",
            "     BatchNorm2d-201         [-1, 1328, 14, 14]           2,656\n",
            "            ReLU-202         [-1, 1328, 14, 14]               0\n",
            "          Conv2d-203           [-1, 32, 14, 14]          42,528\n",
            "     BatchNorm2d-204         [-1, 1360, 14, 14]           2,720\n",
            "            ReLU-205         [-1, 1360, 14, 14]               0\n",
            "          Conv2d-206           [-1, 32, 14, 14]         391,712\n",
            "     BatchNorm2d-207         [-1, 1392, 14, 14]           2,784\n",
            "            ReLU-208         [-1, 1392, 14, 14]               0\n",
            "          Conv2d-209           [-1, 32, 14, 14]          44,576\n",
            "     BatchNorm2d-210         [-1, 1424, 14, 14]           2,848\n",
            "            ReLU-211         [-1, 1424, 14, 14]               0\n",
            "          Conv2d-212           [-1, 32, 14, 14]         410,144\n",
            "     BatchNorm2d-213         [-1, 1456, 14, 14]           2,912\n",
            "            ReLU-214         [-1, 1456, 14, 14]               0\n",
            "          Conv2d-215           [-1, 32, 14, 14]          46,624\n",
            "     BatchNorm2d-216         [-1, 1488, 14, 14]           2,976\n",
            "            ReLU-217         [-1, 1488, 14, 14]               0\n",
            "          Conv2d-218           [-1, 32, 14, 14]         428,576\n",
            "     BatchNorm2d-219         [-1, 1520, 14, 14]           3,040\n",
            "            ReLU-220         [-1, 1520, 14, 14]               0\n",
            "          Conv2d-221           [-1, 32, 14, 14]          48,672\n",
            "     BatchNorm2d-222         [-1, 1552, 14, 14]           3,104\n",
            "            ReLU-223         [-1, 1552, 14, 14]               0\n",
            "          Conv2d-224           [-1, 32, 14, 14]         447,008\n",
            "     BatchNorm2d-225         [-1, 1584, 14, 14]           3,168\n",
            "            ReLU-226         [-1, 1584, 14, 14]               0\n",
            "          Conv2d-227           [-1, 32, 14, 14]          50,720\n",
            "     BatchNorm2d-228         [-1, 1616, 14, 14]           3,232\n",
            "            ReLU-229         [-1, 1616, 14, 14]               0\n",
            "          Conv2d-230           [-1, 32, 14, 14]         465,440\n",
            "     BatchNorm2d-231         [-1, 1648, 14, 14]           3,296\n",
            "            ReLU-232         [-1, 1648, 14, 14]               0\n",
            "          Conv2d-233           [-1, 32, 14, 14]          52,768\n",
            "     BatchNorm2d-234         [-1, 1680, 14, 14]           3,360\n",
            "            ReLU-235         [-1, 1680, 14, 14]               0\n",
            "          Conv2d-236           [-1, 32, 14, 14]         483,872\n",
            "     BatchNorm2d-237         [-1, 1712, 14, 14]           3,424\n",
            "            ReLU-238         [-1, 1712, 14, 14]               0\n",
            "          Conv2d-239           [-1, 32, 14, 14]          54,816\n",
            "     BatchNorm2d-240         [-1, 1744, 14, 14]           3,488\n",
            "            ReLU-241         [-1, 1744, 14, 14]               0\n",
            "          Conv2d-242           [-1, 32, 14, 14]         502,304\n",
            "     BatchNorm2d-243         [-1, 1776, 14, 14]           3,552\n",
            "            ReLU-244         [-1, 1776, 14, 14]               0\n",
            "          Conv2d-245           [-1, 32, 14, 14]          56,864\n",
            "     BatchNorm2d-246         [-1, 1808, 14, 14]           3,616\n",
            "            ReLU-247         [-1, 1808, 14, 14]               0\n",
            "          Conv2d-248           [-1, 32, 14, 14]         520,736\n",
            "     BatchNorm2d-249         [-1, 1840, 14, 14]           3,680\n",
            "            ReLU-250         [-1, 1840, 14, 14]               0\n",
            "          Conv2d-251           [-1, 32, 14, 14]          58,912\n",
            "     BatchNorm2d-252         [-1, 1872, 14, 14]           3,744\n",
            "            ReLU-253         [-1, 1872, 14, 14]               0\n",
            "          Conv2d-254           [-1, 32, 14, 14]         539,168\n",
            "     BatchNorm2d-255         [-1, 1904, 14, 14]           3,808\n",
            "            ReLU-256         [-1, 1904, 14, 14]               0\n",
            "          Conv2d-257           [-1, 32, 14, 14]          60,960\n",
            "     BatchNorm2d-258         [-1, 1936, 14, 14]           3,872\n",
            "            ReLU-259         [-1, 1936, 14, 14]               0\n",
            "          Conv2d-260           [-1, 32, 14, 14]         557,600\n",
            "     BatchNorm2d-261         [-1, 1968, 14, 14]           3,936\n",
            "            ReLU-262         [-1, 1968, 14, 14]               0\n",
            "          Conv2d-263           [-1, 32, 14, 14]          63,008\n",
            "     BatchNorm2d-264         [-1, 2000, 14, 14]           4,000\n",
            "            ReLU-265         [-1, 2000, 14, 14]               0\n",
            "          Conv2d-266           [-1, 32, 14, 14]         576,032\n",
            "      DenseBlock-267         [-1, 2032, 14, 14]               0\n",
            "     BatchNorm2d-268         [-1, 2032, 14, 14]           4,064\n",
            "            ReLU-269         [-1, 2032, 14, 14]               0\n",
            "          Conv2d-270         [-1, 1016, 14, 14]       2,065,528\n",
            "       AvgPool2d-271           [-1, 1016, 7, 7]               0\n",
            "     BatchNorm2d-272           [-1, 1016, 7, 7]           2,032\n",
            "            ReLU-273           [-1, 1016, 7, 7]               0\n",
            "          Conv2d-274             [-1, 32, 7, 7]          32,544\n",
            "     BatchNorm2d-275           [-1, 1048, 7, 7]           2,096\n",
            "            ReLU-276           [-1, 1048, 7, 7]               0\n",
            "          Conv2d-277             [-1, 32, 7, 7]         301,856\n",
            "     BatchNorm2d-278           [-1, 1080, 7, 7]           2,160\n",
            "            ReLU-279           [-1, 1080, 7, 7]               0\n",
            "          Conv2d-280             [-1, 32, 7, 7]          34,592\n",
            "     BatchNorm2d-281           [-1, 1112, 7, 7]           2,224\n",
            "            ReLU-282           [-1, 1112, 7, 7]               0\n",
            "          Conv2d-283             [-1, 32, 7, 7]         320,288\n",
            "     BatchNorm2d-284           [-1, 1144, 7, 7]           2,288\n",
            "            ReLU-285           [-1, 1144, 7, 7]               0\n",
            "          Conv2d-286             [-1, 32, 7, 7]          36,640\n",
            "     BatchNorm2d-287           [-1, 1176, 7, 7]           2,352\n",
            "            ReLU-288           [-1, 1176, 7, 7]               0\n",
            "          Conv2d-289             [-1, 32, 7, 7]         338,720\n",
            "     BatchNorm2d-290           [-1, 1208, 7, 7]           2,416\n",
            "            ReLU-291           [-1, 1208, 7, 7]               0\n",
            "          Conv2d-292             [-1, 32, 7, 7]          38,688\n",
            "     BatchNorm2d-293           [-1, 1240, 7, 7]           2,480\n",
            "            ReLU-294           [-1, 1240, 7, 7]               0\n",
            "          Conv2d-295             [-1, 32, 7, 7]         357,152\n",
            "     BatchNorm2d-296           [-1, 1272, 7, 7]           2,544\n",
            "            ReLU-297           [-1, 1272, 7, 7]               0\n",
            "          Conv2d-298             [-1, 32, 7, 7]          40,736\n",
            "     BatchNorm2d-299           [-1, 1304, 7, 7]           2,608\n",
            "            ReLU-300           [-1, 1304, 7, 7]               0\n",
            "          Conv2d-301             [-1, 32, 7, 7]         375,584\n",
            "     BatchNorm2d-302           [-1, 1336, 7, 7]           2,672\n",
            "            ReLU-303           [-1, 1336, 7, 7]               0\n",
            "          Conv2d-304             [-1, 32, 7, 7]          42,784\n",
            "     BatchNorm2d-305           [-1, 1368, 7, 7]           2,736\n",
            "            ReLU-306           [-1, 1368, 7, 7]               0\n",
            "          Conv2d-307             [-1, 32, 7, 7]         394,016\n",
            "     BatchNorm2d-308           [-1, 1400, 7, 7]           2,800\n",
            "            ReLU-309           [-1, 1400, 7, 7]               0\n",
            "          Conv2d-310             [-1, 32, 7, 7]          44,832\n",
            "     BatchNorm2d-311           [-1, 1432, 7, 7]           2,864\n",
            "            ReLU-312           [-1, 1432, 7, 7]               0\n",
            "          Conv2d-313             [-1, 32, 7, 7]         412,448\n",
            "     BatchNorm2d-314           [-1, 1464, 7, 7]           2,928\n",
            "            ReLU-315           [-1, 1464, 7, 7]               0\n",
            "          Conv2d-316             [-1, 32, 7, 7]          46,880\n",
            "     BatchNorm2d-317           [-1, 1496, 7, 7]           2,992\n",
            "            ReLU-318           [-1, 1496, 7, 7]               0\n",
            "          Conv2d-319             [-1, 32, 7, 7]         430,880\n",
            "     BatchNorm2d-320           [-1, 1528, 7, 7]           3,056\n",
            "            ReLU-321           [-1, 1528, 7, 7]               0\n",
            "          Conv2d-322             [-1, 32, 7, 7]          48,928\n",
            "     BatchNorm2d-323           [-1, 1560, 7, 7]           3,120\n",
            "            ReLU-324           [-1, 1560, 7, 7]               0\n",
            "          Conv2d-325             [-1, 32, 7, 7]         449,312\n",
            "     BatchNorm2d-326           [-1, 1592, 7, 7]           3,184\n",
            "            ReLU-327           [-1, 1592, 7, 7]               0\n",
            "          Conv2d-328             [-1, 32, 7, 7]          50,976\n",
            "     BatchNorm2d-329           [-1, 1624, 7, 7]           3,248\n",
            "            ReLU-330           [-1, 1624, 7, 7]               0\n",
            "          Conv2d-331             [-1, 32, 7, 7]         467,744\n",
            "     BatchNorm2d-332           [-1, 1656, 7, 7]           3,312\n",
            "            ReLU-333           [-1, 1656, 7, 7]               0\n",
            "          Conv2d-334             [-1, 32, 7, 7]          53,024\n",
            "     BatchNorm2d-335           [-1, 1688, 7, 7]           3,376\n",
            "            ReLU-336           [-1, 1688, 7, 7]               0\n",
            "          Conv2d-337             [-1, 32, 7, 7]         486,176\n",
            "     BatchNorm2d-338           [-1, 1720, 7, 7]           3,440\n",
            "            ReLU-339           [-1, 1720, 7, 7]               0\n",
            "          Conv2d-340             [-1, 32, 7, 7]          55,072\n",
            "     BatchNorm2d-341           [-1, 1752, 7, 7]           3,504\n",
            "            ReLU-342           [-1, 1752, 7, 7]               0\n",
            "          Conv2d-343             [-1, 32, 7, 7]         504,608\n",
            "     BatchNorm2d-344           [-1, 1784, 7, 7]           3,568\n",
            "            ReLU-345           [-1, 1784, 7, 7]               0\n",
            "          Conv2d-346             [-1, 32, 7, 7]          57,120\n",
            "     BatchNorm2d-347           [-1, 1816, 7, 7]           3,632\n",
            "            ReLU-348           [-1, 1816, 7, 7]               0\n",
            "          Conv2d-349             [-1, 32, 7, 7]         523,040\n",
            "     BatchNorm2d-350           [-1, 1848, 7, 7]           3,696\n",
            "            ReLU-351           [-1, 1848, 7, 7]               0\n",
            "          Conv2d-352             [-1, 32, 7, 7]          59,168\n",
            "     BatchNorm2d-353           [-1, 1880, 7, 7]           3,760\n",
            "            ReLU-354           [-1, 1880, 7, 7]               0\n",
            "          Conv2d-355             [-1, 32, 7, 7]         541,472\n",
            "     BatchNorm2d-356           [-1, 1912, 7, 7]           3,824\n",
            "            ReLU-357           [-1, 1912, 7, 7]               0\n",
            "          Conv2d-358             [-1, 32, 7, 7]          61,216\n",
            "     BatchNorm2d-359           [-1, 1944, 7, 7]           3,888\n",
            "            ReLU-360           [-1, 1944, 7, 7]               0\n",
            "          Conv2d-361             [-1, 32, 7, 7]         559,904\n",
            "     BatchNorm2d-362           [-1, 1976, 7, 7]           3,952\n",
            "            ReLU-363           [-1, 1976, 7, 7]               0\n",
            "          Conv2d-364             [-1, 32, 7, 7]          63,264\n",
            "     BatchNorm2d-365           [-1, 2008, 7, 7]           4,016\n",
            "            ReLU-366           [-1, 2008, 7, 7]               0\n",
            "          Conv2d-367             [-1, 32, 7, 7]         578,336\n",
            "      DenseBlock-368           [-1, 2040, 7, 7]               0\n",
            "     BatchNorm2d-369           [-1, 2040, 7, 7]           4,080\n",
            "            ReLU-370           [-1, 2040, 7, 7]               0\n",
            "AdaptiveAvgPool2d-371           [-1, 2040, 1, 1]               0\n",
            "         Flatten-372                 [-1, 2040]               0\n",
            "          Linear-373                   [-1, 10]          20,410\n",
            "================================================================\n",
            "Total params: 23,245,586\n",
            "Trainable params: 23,245,586\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 633.18\n",
            "Params size (MB): 88.67\n",
            "Estimated Total Size (MB): 722.05\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price prediction task in Section 5.7."
      ],
      "metadata": {
        "id": "Euh__TdVEfzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Inspirasi dari DenseNet:\n",
        "\n",
        "*  Memanfaatkan konsep dari dense connections di antara layer-layer, di mana setiap layer menerima input dari semua layer sebelumnya.\n",
        "\n",
        "2. Model Architecture:\n",
        "\n",
        "*  Mulai dengan layer input yang menerima fitur-fitur informasi.\n",
        "*  Tambahkan beberapa layer tersembunyi dengan jumlah neuron yang bervariasi.\n",
        "*  Setiap layer baru menggabungkan output dari semua lapisan sebelumnya.\n",
        "\n",
        "3. Data Processing:\n",
        "\n",
        "*  Siapkan dataset dengan membagi data menjadi set pelatihan dan set pengujian.\n",
        "*  Normalisasi atau standarisasi fitur untuk meningkatkan performa model.\n",
        "\n",
        "4. Model Training:\n",
        "\n",
        "*  Gunakan metode optimasi seperti gradient descent untuk melatih model.\n",
        "*  Minimalkan kesalahan prediksi menggunakan fungsi loss, seperti Mean Squared Error (MSE).\n",
        "\n",
        "5. Model Evaluation:\n",
        "\n",
        "*  Uji model pada set pengujian untuk menilai akurasi prediksi.)\n",
        "*  Bandingkan hasil dengan model lain untuk mengevaluasi keefektifan pendekatan DenseNet."
      ],
      "metadata": {
        "id": "HrXanzwTI0z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.8. Designing Convolution Network Architectures"
      ],
      "metadata": {
        "id": "zcZxhxmWWoOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "2nGtdgPbW5F9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.8.1. The AnyNet Design Space"
      ],
      "metadata": {
        "id": "zpkQwFxuYA-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnyNet(d2l.Classifier):\n",
        "    def stem(self, num_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU())"
      ],
      "metadata": {
        "id": "M1d50X-wYGOP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(AnyNet)\n",
        "def stage(self, depth, num_channels, groups, bot_mul):\n",
        "    blk = []\n",
        "    for i in range(depth):\n",
        "        if i == 0:\n",
        "            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\n",
        "                use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))\n",
        "    return nn.Sequential(*blk)"
      ],
      "metadata": {
        "id": "LZUZY6z2YIB3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(AnyNet)\n",
        "def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n",
        "    super(AnyNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.stem(stem_channels))\n",
        "    for i, s in enumerate(arch):\n",
        "        self.net.add_module(f'stage{i+1}', self.stage(*s))\n",
        "    self.net.add_module('head', nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "yR2QIRsHYKhv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.8.3. RegNet"
      ],
      "metadata": {
        "id": "0Rq5JDWxYN7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegNetX32(AnyNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        stem_channels, groups, bot_mul = 32, 16, 1\n",
        "        depths, channels = (4, 6), (32, 80)\n",
        "        super().__init__(\n",
        "            ((depths[0], channels[0], groups, bot_mul),\n",
        "             (depths[1], channels[1], groups, bot_mul)),\n",
        "            stem_channels, lr, num_classes)"
      ],
      "metadata": {
        "id": "pzYneyTpYQIY"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RegNetX32().layer_summary((1, 1, 96, 96))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1pu08b4YR5G",
        "outputId": "46c0ca14-9e01-467a-c148-0e78f8d63832"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 32, 48, 48])\n",
            "Sequential output shape:\t torch.Size([1, 32, 24, 24])\n",
            "Sequential output shape:\t torch.Size([1, 80, 12, 12])\n",
            "Sequential output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.8.4. Training"
      ],
      "metadata": {
        "id": "ngQ6kfumYUcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RegNetX32(lr=0.05)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "5sG1q8U5YXxM",
        "outputId": "e2a5971a-a040-488e-aadf-57b6d275bfd3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-e549fd465cea>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFashionMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/d2l/torch.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_batch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/d2l/torch.py\u001b[0m in \u001b[0;36mfit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/d2l/torch.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/d2l/torch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'net'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Neural network is defined'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/d2l/torch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \"\"\"\n\u001b[0;32m--> 176\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2510\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2512\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2513\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2514\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.8.6. Exercises"
      ],
      "metadata": {
        "id": "9FFbFjxcLVVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Increase the number of stages to four. Can you design a deeper RegNetX that performs better?"
      ],
      "metadata": {
        "id": "-TShbWT6LgtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "class AnyNet(nn.Module):\n",
        "    def stem(self, num_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(3, num_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(num_channels), nn.ReLU())\n",
        "\n",
        "    def stage(self, depth, num_channels, prev_channels, groups, bot_mul):\n",
        "        blk = []\n",
        "        if num_channels != prev_channels:  # Jika jumlah channels berubah, tambahkan 1x1 convolution\n",
        "            blk.append(nn.Conv2d(prev_channels, num_channels, kernel_size=1, stride=1))\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                blk.append(self.ResNeXtBlock(num_channels, groups, bot_mul, use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                blk.append(self.ResNeXtBlock(num_channels, groups, bot_mul))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n",
        "        super(AnyNet, self).__init__()\n",
        "        self.net = nn.Sequential(self.stem(stem_channels))\n",
        "        prev_channels = stem_channels  # Menyimpan jumlah channel sebelumnya untuk peralihan stage\n",
        "        for i, s in enumerate(arch):\n",
        "            self.net.add_module(f'stage{i+1}', self.stage(s[0], s[1], prev_channels, s[2], s[3]))\n",
        "            prev_channels = s[1]  # Update jumlah channels setelah stage\n",
        "        self.net.add_module('head', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.Linear(prev_channels, num_classes)))  # Menggunakan channels dari stage terakhir\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    class ResNeXtBlock(nn.Module):\n",
        "        def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False, strides=1):\n",
        "            super().__init__()\n",
        "            mid_channels = int(num_channels * bot_mul)\n",
        "            self.conv1 = nn.Conv2d(num_channels, mid_channels, kernel_size=1, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "            self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=strides,\n",
        "                                   padding=1, groups=groups, bias=False)\n",
        "            self.bn2 = nn.BatchNorm2d(mid_channels)\n",
        "            self.conv3 = nn.Conv2d(mid_channels, num_channels, kernel_size=1, bias=False)\n",
        "            self.bn3 = nn.BatchNorm2d(num_channels)\n",
        "            if use_1x1conv:\n",
        "                self.conv1x1 = nn.Conv2d(num_channels, num_channels, kernel_size=1, stride=strides, bias=False)\n",
        "                self.bn1x1 = nn.BatchNorm2d(num_channels)\n",
        "            else:\n",
        "                self.conv1x1 = None\n",
        "\n",
        "        def forward(self, x):\n",
        "            residual = x\n",
        "            out = F.relu(self.bn1(self.conv1(x)))\n",
        "            out = F.relu(self.bn2(self.conv2(out)))\n",
        "            out = self.bn3(self.conv3(out))\n",
        "\n",
        "            if self.conv1x1:\n",
        "                residual = self.bn1x1(self.conv1x1(x))\n",
        "\n",
        "            return F.relu(out + residual)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Definisikan RegNetX yang lebih dalam\n",
        "class RegNetXDeep(AnyNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        stem_channels, groups, bot_mul = 32, 16, 1\n",
        "        # Kedalaman dan jumlah channel meningkat pada empat stages\n",
        "        depths, channels = (6, 8, 12, 18), (32, 80, 128, 256)\n",
        "        super().__init__(\n",
        "            [(depths[i], channels[i], groups, bot_mul) for i in range(len(depths))],\n",
        "            stem_channels, lr, num_classes)\n",
        "\n",
        "# Contoh untuk mengecek model\n",
        "model = RegNetXDeep()\n",
        "summary(model, (3, 224, 224))  # Ukuran input 3x224x224"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4S7QtpFPytM",
        "outputId": "dbb045c1-e536-4d4e-8180-a5449c84e0f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 112, 112]             896\n",
            "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
            "              ReLU-3         [-1, 32, 112, 112]               0\n",
            "            Conv2d-4         [-1, 32, 112, 112]           1,024\n",
            "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
            "            Conv2d-6           [-1, 32, 56, 56]             576\n",
            "       BatchNorm2d-7           [-1, 32, 56, 56]              64\n",
            "            Conv2d-8           [-1, 32, 56, 56]           1,024\n",
            "       BatchNorm2d-9           [-1, 32, 56, 56]              64\n",
            "           Conv2d-10           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-11           [-1, 32, 56, 56]              64\n",
            "     ResNeXtBlock-12           [-1, 32, 56, 56]               0\n",
            "           Conv2d-13           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-14           [-1, 32, 56, 56]              64\n",
            "           Conv2d-15           [-1, 32, 56, 56]             576\n",
            "      BatchNorm2d-16           [-1, 32, 56, 56]              64\n",
            "           Conv2d-17           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-18           [-1, 32, 56, 56]              64\n",
            "     ResNeXtBlock-19           [-1, 32, 56, 56]               0\n",
            "           Conv2d-20           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-21           [-1, 32, 56, 56]              64\n",
            "           Conv2d-22           [-1, 32, 56, 56]             576\n",
            "      BatchNorm2d-23           [-1, 32, 56, 56]              64\n",
            "           Conv2d-24           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-25           [-1, 32, 56, 56]              64\n",
            "     ResNeXtBlock-26           [-1, 32, 56, 56]               0\n",
            "           Conv2d-27           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-28           [-1, 32, 56, 56]              64\n",
            "           Conv2d-29           [-1, 32, 56, 56]             576\n",
            "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
            "           Conv2d-31           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-32           [-1, 32, 56, 56]              64\n",
            "     ResNeXtBlock-33           [-1, 32, 56, 56]               0\n",
            "           Conv2d-34           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-35           [-1, 32, 56, 56]              64\n",
            "           Conv2d-36           [-1, 32, 56, 56]             576\n",
            "      BatchNorm2d-37           [-1, 32, 56, 56]              64\n",
            "           Conv2d-38           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-39           [-1, 32, 56, 56]              64\n",
            "     ResNeXtBlock-40           [-1, 32, 56, 56]               0\n",
            "           Conv2d-41           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-42           [-1, 32, 56, 56]              64\n",
            "           Conv2d-43           [-1, 32, 56, 56]             576\n",
            "      BatchNorm2d-44           [-1, 32, 56, 56]              64\n",
            "           Conv2d-45           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-46           [-1, 32, 56, 56]              64\n",
            "     ResNeXtBlock-47           [-1, 32, 56, 56]               0\n",
            "           Conv2d-48           [-1, 80, 56, 56]           2,640\n",
            "           Conv2d-49           [-1, 80, 56, 56]           6,400\n",
            "      BatchNorm2d-50           [-1, 80, 56, 56]             160\n",
            "           Conv2d-51           [-1, 80, 28, 28]           3,600\n",
            "      BatchNorm2d-52           [-1, 80, 28, 28]             160\n",
            "           Conv2d-53           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-54           [-1, 80, 28, 28]             160\n",
            "           Conv2d-55           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-56           [-1, 80, 28, 28]             160\n",
            "     ResNeXtBlock-57           [-1, 80, 28, 28]               0\n",
            "           Conv2d-58           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-59           [-1, 80, 28, 28]             160\n",
            "           Conv2d-60           [-1, 80, 28, 28]           3,600\n",
            "      BatchNorm2d-61           [-1, 80, 28, 28]             160\n",
            "           Conv2d-62           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-63           [-1, 80, 28, 28]             160\n",
            "     ResNeXtBlock-64           [-1, 80, 28, 28]               0\n",
            "           Conv2d-65           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-66           [-1, 80, 28, 28]             160\n",
            "           Conv2d-67           [-1, 80, 28, 28]           3,600\n",
            "      BatchNorm2d-68           [-1, 80, 28, 28]             160\n",
            "           Conv2d-69           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-70           [-1, 80, 28, 28]             160\n",
            "     ResNeXtBlock-71           [-1, 80, 28, 28]               0\n",
            "           Conv2d-72           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-73           [-1, 80, 28, 28]             160\n",
            "           Conv2d-74           [-1, 80, 28, 28]           3,600\n",
            "      BatchNorm2d-75           [-1, 80, 28, 28]             160\n",
            "           Conv2d-76           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-77           [-1, 80, 28, 28]             160\n",
            "     ResNeXtBlock-78           [-1, 80, 28, 28]               0\n",
            "           Conv2d-79           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-80           [-1, 80, 28, 28]             160\n",
            "           Conv2d-81           [-1, 80, 28, 28]           3,600\n",
            "      BatchNorm2d-82           [-1, 80, 28, 28]             160\n",
            "           Conv2d-83           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-84           [-1, 80, 28, 28]             160\n",
            "     ResNeXtBlock-85           [-1, 80, 28, 28]               0\n",
            "           Conv2d-86           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-87           [-1, 80, 28, 28]             160\n",
            "           Conv2d-88           [-1, 80, 28, 28]           3,600\n",
            "      BatchNorm2d-89           [-1, 80, 28, 28]             160\n",
            "           Conv2d-90           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-91           [-1, 80, 28, 28]             160\n",
            "     ResNeXtBlock-92           [-1, 80, 28, 28]               0\n",
            "           Conv2d-93           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-94           [-1, 80, 28, 28]             160\n",
            "           Conv2d-95           [-1, 80, 28, 28]           3,600\n",
            "      BatchNorm2d-96           [-1, 80, 28, 28]             160\n",
            "           Conv2d-97           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-98           [-1, 80, 28, 28]             160\n",
            "     ResNeXtBlock-99           [-1, 80, 28, 28]               0\n",
            "          Conv2d-100           [-1, 80, 28, 28]           6,400\n",
            "     BatchNorm2d-101           [-1, 80, 28, 28]             160\n",
            "          Conv2d-102           [-1, 80, 28, 28]           3,600\n",
            "     BatchNorm2d-103           [-1, 80, 28, 28]             160\n",
            "          Conv2d-104           [-1, 80, 28, 28]           6,400\n",
            "     BatchNorm2d-105           [-1, 80, 28, 28]             160\n",
            "    ResNeXtBlock-106           [-1, 80, 28, 28]               0\n",
            "          Conv2d-107          [-1, 128, 28, 28]          10,368\n",
            "          Conv2d-108          [-1, 128, 28, 28]          16,384\n",
            "     BatchNorm2d-109          [-1, 128, 28, 28]             256\n",
            "          Conv2d-110          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-111          [-1, 128, 14, 14]             256\n",
            "          Conv2d-112          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-113          [-1, 128, 14, 14]             256\n",
            "          Conv2d-114          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-115          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-116          [-1, 128, 14, 14]               0\n",
            "          Conv2d-117          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-118          [-1, 128, 14, 14]             256\n",
            "          Conv2d-119          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-120          [-1, 128, 14, 14]             256\n",
            "          Conv2d-121          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-122          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-123          [-1, 128, 14, 14]               0\n",
            "          Conv2d-124          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-125          [-1, 128, 14, 14]             256\n",
            "          Conv2d-126          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-127          [-1, 128, 14, 14]             256\n",
            "          Conv2d-128          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-129          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-130          [-1, 128, 14, 14]               0\n",
            "          Conv2d-131          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-132          [-1, 128, 14, 14]             256\n",
            "          Conv2d-133          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-134          [-1, 128, 14, 14]             256\n",
            "          Conv2d-135          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-136          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-137          [-1, 128, 14, 14]               0\n",
            "          Conv2d-138          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-139          [-1, 128, 14, 14]             256\n",
            "          Conv2d-140          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-141          [-1, 128, 14, 14]             256\n",
            "          Conv2d-142          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-143          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-144          [-1, 128, 14, 14]               0\n",
            "          Conv2d-145          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-146          [-1, 128, 14, 14]             256\n",
            "          Conv2d-147          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-148          [-1, 128, 14, 14]             256\n",
            "          Conv2d-149          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-150          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-151          [-1, 128, 14, 14]               0\n",
            "          Conv2d-152          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-153          [-1, 128, 14, 14]             256\n",
            "          Conv2d-154          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-155          [-1, 128, 14, 14]             256\n",
            "          Conv2d-156          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-157          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-158          [-1, 128, 14, 14]               0\n",
            "          Conv2d-159          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-160          [-1, 128, 14, 14]             256\n",
            "          Conv2d-161          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-162          [-1, 128, 14, 14]             256\n",
            "          Conv2d-163          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-164          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-165          [-1, 128, 14, 14]               0\n",
            "          Conv2d-166          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-167          [-1, 128, 14, 14]             256\n",
            "          Conv2d-168          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-169          [-1, 128, 14, 14]             256\n",
            "          Conv2d-170          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-171          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-172          [-1, 128, 14, 14]               0\n",
            "          Conv2d-173          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-174          [-1, 128, 14, 14]             256\n",
            "          Conv2d-175          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-176          [-1, 128, 14, 14]             256\n",
            "          Conv2d-177          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-178          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-179          [-1, 128, 14, 14]               0\n",
            "          Conv2d-180          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-181          [-1, 128, 14, 14]             256\n",
            "          Conv2d-182          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-183          [-1, 128, 14, 14]             256\n",
            "          Conv2d-184          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-185          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-186          [-1, 128, 14, 14]               0\n",
            "          Conv2d-187          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-188          [-1, 128, 14, 14]             256\n",
            "          Conv2d-189          [-1, 128, 14, 14]           9,216\n",
            "     BatchNorm2d-190          [-1, 128, 14, 14]             256\n",
            "          Conv2d-191          [-1, 128, 14, 14]          16,384\n",
            "     BatchNorm2d-192          [-1, 128, 14, 14]             256\n",
            "    ResNeXtBlock-193          [-1, 128, 14, 14]               0\n",
            "          Conv2d-194          [-1, 256, 14, 14]          33,024\n",
            "          Conv2d-195          [-1, 256, 14, 14]          65,536\n",
            "     BatchNorm2d-196          [-1, 256, 14, 14]             512\n",
            "          Conv2d-197            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-198            [-1, 256, 7, 7]             512\n",
            "          Conv2d-199            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-200            [-1, 256, 7, 7]             512\n",
            "          Conv2d-201            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-202            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-203            [-1, 256, 7, 7]               0\n",
            "          Conv2d-204            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-205            [-1, 256, 7, 7]             512\n",
            "          Conv2d-206            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-207            [-1, 256, 7, 7]             512\n",
            "          Conv2d-208            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-209            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-210            [-1, 256, 7, 7]               0\n",
            "          Conv2d-211            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-212            [-1, 256, 7, 7]             512\n",
            "          Conv2d-213            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-214            [-1, 256, 7, 7]             512\n",
            "          Conv2d-215            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-216            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-217            [-1, 256, 7, 7]               0\n",
            "          Conv2d-218            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-219            [-1, 256, 7, 7]             512\n",
            "          Conv2d-220            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-221            [-1, 256, 7, 7]             512\n",
            "          Conv2d-222            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-223            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-224            [-1, 256, 7, 7]               0\n",
            "          Conv2d-225            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-226            [-1, 256, 7, 7]             512\n",
            "          Conv2d-227            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-228            [-1, 256, 7, 7]             512\n",
            "          Conv2d-229            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-230            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-231            [-1, 256, 7, 7]               0\n",
            "          Conv2d-232            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-233            [-1, 256, 7, 7]             512\n",
            "          Conv2d-234            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-235            [-1, 256, 7, 7]             512\n",
            "          Conv2d-236            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-237            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-238            [-1, 256, 7, 7]               0\n",
            "          Conv2d-239            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-240            [-1, 256, 7, 7]             512\n",
            "          Conv2d-241            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-242            [-1, 256, 7, 7]             512\n",
            "          Conv2d-243            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-244            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-245            [-1, 256, 7, 7]               0\n",
            "          Conv2d-246            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-247            [-1, 256, 7, 7]             512\n",
            "          Conv2d-248            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-249            [-1, 256, 7, 7]             512\n",
            "          Conv2d-250            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-251            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-252            [-1, 256, 7, 7]               0\n",
            "          Conv2d-253            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-254            [-1, 256, 7, 7]             512\n",
            "          Conv2d-255            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-256            [-1, 256, 7, 7]             512\n",
            "          Conv2d-257            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-258            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-259            [-1, 256, 7, 7]               0\n",
            "          Conv2d-260            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-261            [-1, 256, 7, 7]             512\n",
            "          Conv2d-262            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-263            [-1, 256, 7, 7]             512\n",
            "          Conv2d-264            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-265            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-266            [-1, 256, 7, 7]               0\n",
            "          Conv2d-267            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-268            [-1, 256, 7, 7]             512\n",
            "          Conv2d-269            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-270            [-1, 256, 7, 7]             512\n",
            "          Conv2d-271            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-272            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-273            [-1, 256, 7, 7]               0\n",
            "          Conv2d-274            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-275            [-1, 256, 7, 7]             512\n",
            "          Conv2d-276            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-277            [-1, 256, 7, 7]             512\n",
            "          Conv2d-278            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-279            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-280            [-1, 256, 7, 7]               0\n",
            "          Conv2d-281            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-282            [-1, 256, 7, 7]             512\n",
            "          Conv2d-283            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-284            [-1, 256, 7, 7]             512\n",
            "          Conv2d-285            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-286            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-287            [-1, 256, 7, 7]               0\n",
            "          Conv2d-288            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-289            [-1, 256, 7, 7]             512\n",
            "          Conv2d-290            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-291            [-1, 256, 7, 7]             512\n",
            "          Conv2d-292            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-293            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-294            [-1, 256, 7, 7]               0\n",
            "          Conv2d-295            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-296            [-1, 256, 7, 7]             512\n",
            "          Conv2d-297            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-298            [-1, 256, 7, 7]             512\n",
            "          Conv2d-299            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-300            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-301            [-1, 256, 7, 7]               0\n",
            "          Conv2d-302            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-303            [-1, 256, 7, 7]             512\n",
            "          Conv2d-304            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-305            [-1, 256, 7, 7]             512\n",
            "          Conv2d-306            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-307            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-308            [-1, 256, 7, 7]               0\n",
            "          Conv2d-309            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-310            [-1, 256, 7, 7]             512\n",
            "          Conv2d-311            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-312            [-1, 256, 7, 7]             512\n",
            "          Conv2d-313            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-314            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-315            [-1, 256, 7, 7]               0\n",
            "          Conv2d-316            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-317            [-1, 256, 7, 7]             512\n",
            "          Conv2d-318            [-1, 256, 7, 7]          36,864\n",
            "     BatchNorm2d-319            [-1, 256, 7, 7]             512\n",
            "          Conv2d-320            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-321            [-1, 256, 7, 7]             512\n",
            "    ResNeXtBlock-322            [-1, 256, 7, 7]               0\n",
            "AdaptiveAvgPool2d-323            [-1, 256, 1, 1]               0\n",
            "         Flatten-324                  [-1, 256]               0\n",
            "          Linear-325                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 3,855,354\n",
            "Trainable params: 3,855,354\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 111.59\n",
            "Params size (MB): 14.71\n",
            "Estimated Total Size (MB): 126.88\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. De-ResNeXt-ify RegNets by replacing the ResNeXt block with the ResNet block. How does your new model perform?"
      ],
      "metadata": {
        "id": "y3iYFAX0Qa_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "class AnyNet(nn.Module):\n",
        "    def stem(self, num_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(3, num_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(num_channels), nn.ReLU())\n",
        "\n",
        "    def stage(self, depth, num_channels, prev_channels, use_1x1conv=False):\n",
        "        blk = []\n",
        "        if num_channels != prev_channels:  # Jika jumlah channels berubah, tambahkan 1x1 convolution\n",
        "            blk.append(nn.Conv2d(prev_channels, num_channels, kernel_size=1, stride=1))\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                blk.append(self.ResNetBlock(num_channels, use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                blk.append(self.ResNetBlock(num_channels))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n",
        "        super(AnyNet, self).__init__()\n",
        "        self.net = nn.Sequential(self.stem(stem_channels))\n",
        "        prev_channels = stem_channels  # Menyimpan jumlah channel sebelumnya untuk peralihan stage\n",
        "        for i, s in enumerate(arch):\n",
        "            self.net.add_module(f'stage{i+1}', self.stage(s[0], s[1], prev_channels))\n",
        "            prev_channels = s[1]  # Update jumlah channels setelah stage\n",
        "        self.net.add_module('head', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.Linear(prev_channels, num_classes)))  # Menggunakan channels dari stage terakhir\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    class ResNetBlock(nn.Module):\n",
        "        def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=strides, padding=1, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "            self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "            if use_1x1conv:\n",
        "                self.conv1x1 = nn.Conv2d(num_channels, num_channels, kernel_size=1, stride=strides, bias=False)\n",
        "                self.bn1x1 = nn.BatchNorm2d(num_channels)\n",
        "            else:\n",
        "                self.conv1x1 = None\n",
        "\n",
        "        def forward(self, x):\n",
        "            residual = x\n",
        "            out = F.relu(self.bn1(self.conv1(x)))\n",
        "            out = self.bn2(self.conv2(out))\n",
        "\n",
        "            if self.conv1x1:\n",
        "                residual = self.bn1x1(self.conv1x1(x))\n",
        "\n",
        "            return F.relu(out + residual)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Definisikan RegNet yang lebih dalam dengan ResNetBlock\n",
        "class RegNetXResNet(AnyNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        stem_channels = 32\n",
        "        # Kedalaman dan jumlah channel meningkat pada empat stages\n",
        "        depths, channels = (6, 8, 12, 18), (32, 80, 128, 256)\n",
        "        super().__init__(\n",
        "            [(depths[i], channels[i], False) for i in range(len(depths))],\n",
        "            stem_channels, lr, num_classes)\n",
        "\n",
        "# Contoh untuk mengecek model\n",
        "model = RegNetXResNet()\n",
        "summary(model, (3, 224, 224))  # Ukuran input 3x224x224"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D0XbmxKPzBi",
        "outputId": "ef7b5e34-152b-4b57-91be-7a4ba173c3ed"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 112, 112]             896\n",
            "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
            "              ReLU-3         [-1, 32, 112, 112]               0\n",
            "            Conv2d-4           [-1, 32, 56, 56]           9,216\n",
            "       BatchNorm2d-5           [-1, 32, 56, 56]              64\n",
            "            Conv2d-6           [-1, 32, 56, 56]           9,216\n",
            "       BatchNorm2d-7           [-1, 32, 56, 56]              64\n",
            "            Conv2d-8           [-1, 32, 56, 56]           1,024\n",
            "       BatchNorm2d-9           [-1, 32, 56, 56]              64\n",
            "      ResNetBlock-10           [-1, 32, 56, 56]               0\n",
            "           Conv2d-11           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-12           [-1, 32, 56, 56]              64\n",
            "           Conv2d-13           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-14           [-1, 32, 56, 56]              64\n",
            "      ResNetBlock-15           [-1, 32, 56, 56]               0\n",
            "           Conv2d-16           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-17           [-1, 32, 56, 56]              64\n",
            "           Conv2d-18           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-19           [-1, 32, 56, 56]              64\n",
            "      ResNetBlock-20           [-1, 32, 56, 56]               0\n",
            "           Conv2d-21           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-22           [-1, 32, 56, 56]              64\n",
            "           Conv2d-23           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
            "      ResNetBlock-25           [-1, 32, 56, 56]               0\n",
            "           Conv2d-26           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-27           [-1, 32, 56, 56]              64\n",
            "           Conv2d-28           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-29           [-1, 32, 56, 56]              64\n",
            "      ResNetBlock-30           [-1, 32, 56, 56]               0\n",
            "           Conv2d-31           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-32           [-1, 32, 56, 56]              64\n",
            "           Conv2d-33           [-1, 32, 56, 56]           9,216\n",
            "      BatchNorm2d-34           [-1, 32, 56, 56]              64\n",
            "      ResNetBlock-35           [-1, 32, 56, 56]               0\n",
            "           Conv2d-36           [-1, 80, 56, 56]           2,640\n",
            "           Conv2d-37           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-38           [-1, 80, 28, 28]             160\n",
            "           Conv2d-39           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-40           [-1, 80, 28, 28]             160\n",
            "           Conv2d-41           [-1, 80, 28, 28]           6,400\n",
            "      BatchNorm2d-42           [-1, 80, 28, 28]             160\n",
            "      ResNetBlock-43           [-1, 80, 28, 28]               0\n",
            "           Conv2d-44           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-45           [-1, 80, 28, 28]             160\n",
            "           Conv2d-46           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-47           [-1, 80, 28, 28]             160\n",
            "      ResNetBlock-48           [-1, 80, 28, 28]               0\n",
            "           Conv2d-49           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-50           [-1, 80, 28, 28]             160\n",
            "           Conv2d-51           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-52           [-1, 80, 28, 28]             160\n",
            "      ResNetBlock-53           [-1, 80, 28, 28]               0\n",
            "           Conv2d-54           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-55           [-1, 80, 28, 28]             160\n",
            "           Conv2d-56           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-57           [-1, 80, 28, 28]             160\n",
            "      ResNetBlock-58           [-1, 80, 28, 28]               0\n",
            "           Conv2d-59           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-60           [-1, 80, 28, 28]             160\n",
            "           Conv2d-61           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-62           [-1, 80, 28, 28]             160\n",
            "      ResNetBlock-63           [-1, 80, 28, 28]               0\n",
            "           Conv2d-64           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-65           [-1, 80, 28, 28]             160\n",
            "           Conv2d-66           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-67           [-1, 80, 28, 28]             160\n",
            "      ResNetBlock-68           [-1, 80, 28, 28]               0\n",
            "           Conv2d-69           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-70           [-1, 80, 28, 28]             160\n",
            "           Conv2d-71           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-72           [-1, 80, 28, 28]             160\n",
            "      ResNetBlock-73           [-1, 80, 28, 28]               0\n",
            "           Conv2d-74           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-75           [-1, 80, 28, 28]             160\n",
            "           Conv2d-76           [-1, 80, 28, 28]          57,600\n",
            "      BatchNorm2d-77           [-1, 80, 28, 28]             160\n",
            "      ResNetBlock-78           [-1, 80, 28, 28]               0\n",
            "           Conv2d-79          [-1, 128, 28, 28]          10,368\n",
            "           Conv2d-80          [-1, 128, 14, 14]         147,456\n",
            "      BatchNorm2d-81          [-1, 128, 14, 14]             256\n",
            "           Conv2d-82          [-1, 128, 14, 14]         147,456\n",
            "      BatchNorm2d-83          [-1, 128, 14, 14]             256\n",
            "           Conv2d-84          [-1, 128, 14, 14]          16,384\n",
            "      BatchNorm2d-85          [-1, 128, 14, 14]             256\n",
            "      ResNetBlock-86          [-1, 128, 14, 14]               0\n",
            "           Conv2d-87          [-1, 128, 14, 14]         147,456\n",
            "      BatchNorm2d-88          [-1, 128, 14, 14]             256\n",
            "           Conv2d-89          [-1, 128, 14, 14]         147,456\n",
            "      BatchNorm2d-90          [-1, 128, 14, 14]             256\n",
            "      ResNetBlock-91          [-1, 128, 14, 14]               0\n",
            "           Conv2d-92          [-1, 128, 14, 14]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 14, 14]             256\n",
            "           Conv2d-94          [-1, 128, 14, 14]         147,456\n",
            "      BatchNorm2d-95          [-1, 128, 14, 14]             256\n",
            "      ResNetBlock-96          [-1, 128, 14, 14]               0\n",
            "           Conv2d-97          [-1, 128, 14, 14]         147,456\n",
            "      BatchNorm2d-98          [-1, 128, 14, 14]             256\n",
            "           Conv2d-99          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-100          [-1, 128, 14, 14]             256\n",
            "     ResNetBlock-101          [-1, 128, 14, 14]               0\n",
            "          Conv2d-102          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-103          [-1, 128, 14, 14]             256\n",
            "          Conv2d-104          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-105          [-1, 128, 14, 14]             256\n",
            "     ResNetBlock-106          [-1, 128, 14, 14]               0\n",
            "          Conv2d-107          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-108          [-1, 128, 14, 14]             256\n",
            "          Conv2d-109          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-110          [-1, 128, 14, 14]             256\n",
            "     ResNetBlock-111          [-1, 128, 14, 14]               0\n",
            "          Conv2d-112          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-113          [-1, 128, 14, 14]             256\n",
            "          Conv2d-114          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-115          [-1, 128, 14, 14]             256\n",
            "     ResNetBlock-116          [-1, 128, 14, 14]               0\n",
            "          Conv2d-117          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-118          [-1, 128, 14, 14]             256\n",
            "          Conv2d-119          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-120          [-1, 128, 14, 14]             256\n",
            "     ResNetBlock-121          [-1, 128, 14, 14]               0\n",
            "          Conv2d-122          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-123          [-1, 128, 14, 14]             256\n",
            "          Conv2d-124          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-125          [-1, 128, 14, 14]             256\n",
            "     ResNetBlock-126          [-1, 128, 14, 14]               0\n",
            "          Conv2d-127          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-128          [-1, 128, 14, 14]             256\n",
            "          Conv2d-129          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-130          [-1, 128, 14, 14]             256\n",
            "     ResNetBlock-131          [-1, 128, 14, 14]               0\n",
            "          Conv2d-132          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-133          [-1, 128, 14, 14]             256\n",
            "          Conv2d-134          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-135          [-1, 128, 14, 14]             256\n",
            "     ResNetBlock-136          [-1, 128, 14, 14]               0\n",
            "          Conv2d-137          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-138          [-1, 128, 14, 14]             256\n",
            "          Conv2d-139          [-1, 128, 14, 14]         147,456\n",
            "     BatchNorm2d-140          [-1, 128, 14, 14]             256\n",
            "     ResNetBlock-141          [-1, 128, 14, 14]               0\n",
            "          Conv2d-142          [-1, 256, 14, 14]          33,024\n",
            "          Conv2d-143            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-144            [-1, 256, 7, 7]             512\n",
            "          Conv2d-145            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-146            [-1, 256, 7, 7]             512\n",
            "          Conv2d-147            [-1, 256, 7, 7]          65,536\n",
            "     BatchNorm2d-148            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-149            [-1, 256, 7, 7]               0\n",
            "          Conv2d-150            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-151            [-1, 256, 7, 7]             512\n",
            "          Conv2d-152            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-153            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-154            [-1, 256, 7, 7]               0\n",
            "          Conv2d-155            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-156            [-1, 256, 7, 7]             512\n",
            "          Conv2d-157            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-158            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-159            [-1, 256, 7, 7]               0\n",
            "          Conv2d-160            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-161            [-1, 256, 7, 7]             512\n",
            "          Conv2d-162            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-163            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-164            [-1, 256, 7, 7]               0\n",
            "          Conv2d-165            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-166            [-1, 256, 7, 7]             512\n",
            "          Conv2d-167            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-168            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-169            [-1, 256, 7, 7]               0\n",
            "          Conv2d-170            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-171            [-1, 256, 7, 7]             512\n",
            "          Conv2d-172            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-173            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-174            [-1, 256, 7, 7]               0\n",
            "          Conv2d-175            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-176            [-1, 256, 7, 7]             512\n",
            "          Conv2d-177            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-178            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-179            [-1, 256, 7, 7]               0\n",
            "          Conv2d-180            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-181            [-1, 256, 7, 7]             512\n",
            "          Conv2d-182            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-183            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-184            [-1, 256, 7, 7]               0\n",
            "          Conv2d-185            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-186            [-1, 256, 7, 7]             512\n",
            "          Conv2d-187            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-188            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-189            [-1, 256, 7, 7]               0\n",
            "          Conv2d-190            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-191            [-1, 256, 7, 7]             512\n",
            "          Conv2d-192            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-193            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-194            [-1, 256, 7, 7]               0\n",
            "          Conv2d-195            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-196            [-1, 256, 7, 7]             512\n",
            "          Conv2d-197            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-198            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-199            [-1, 256, 7, 7]               0\n",
            "          Conv2d-200            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-201            [-1, 256, 7, 7]             512\n",
            "          Conv2d-202            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-203            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-204            [-1, 256, 7, 7]               0\n",
            "          Conv2d-205            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-206            [-1, 256, 7, 7]             512\n",
            "          Conv2d-207            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-208            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-209            [-1, 256, 7, 7]               0\n",
            "          Conv2d-210            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-211            [-1, 256, 7, 7]             512\n",
            "          Conv2d-212            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-213            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-214            [-1, 256, 7, 7]               0\n",
            "          Conv2d-215            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-216            [-1, 256, 7, 7]             512\n",
            "          Conv2d-217            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-218            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-219            [-1, 256, 7, 7]               0\n",
            "          Conv2d-220            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-221            [-1, 256, 7, 7]             512\n",
            "          Conv2d-222            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-223            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-224            [-1, 256, 7, 7]               0\n",
            "          Conv2d-225            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-226            [-1, 256, 7, 7]             512\n",
            "          Conv2d-227            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-228            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-229            [-1, 256, 7, 7]               0\n",
            "          Conv2d-230            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-231            [-1, 256, 7, 7]             512\n",
            "          Conv2d-232            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-233            [-1, 256, 7, 7]             512\n",
            "     ResNetBlock-234            [-1, 256, 7, 7]               0\n",
            "AdaptiveAvgPool2d-235            [-1, 256, 1, 1]               0\n",
            "         Flatten-236                  [-1, 256]               0\n",
            "          Linear-237                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 25,972,602\n",
            "Trainable params: 25,972,602\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 77.52\n",
            "Params size (MB): 99.08\n",
            "Estimated Total Size (MB): 177.18\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "_csGmbxiQvvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "class VioNetBlock(nn.Module):\n",
        "    def __init__(self, num_channels, kernel_size=3, use_1x1conv=False, strides=1, skip_connections=True):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=kernel_size, stride=strides, padding=kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "        self.skip_connections = skip_connections\n",
        "        if use_1x1conv:\n",
        "            self.conv1x1 = nn.Conv2d(num_channels, num_channels, kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv1x1 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.skip_connections and self.conv1x1:\n",
        "            residual = self.conv1x1(x)\n",
        "\n",
        "        if self.skip_connections:\n",
        "            return F.relu(out + residual)\n",
        "        else:\n",
        "            return F.relu(out)\n",
        "\n",
        "class VioNet(nn.Module):\n",
        "    def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n",
        "        super(VioNet, self).__init__()\n",
        "        self.net = nn.Sequential(self.stem(stem_channels))\n",
        "        prev_channels = stem_channels\n",
        "        for i, s in enumerate(arch):\n",
        "            self.net.add_module(f'stage{i+1}', self.stage(s[0], s[1], s[2], prev_channels))\n",
        "            prev_channels = s[1]\n",
        "        self.net.add_module('head', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.Linear(prev_channels, num_classes)))\n",
        "        self.init_weights()\n",
        "\n",
        "    def stem(self, num_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(3, num_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(num_channels), nn.ReLU())\n",
        "\n",
        "    def stage(self, depth, num_channels, kernel_size, prev_channels):\n",
        "        blk = []\n",
        "        if num_channels != prev_channels:\n",
        "            blk.append(nn.Conv2d(prev_channels, num_channels, kernel_size=1))\n",
        "        for i in range(depth):\n",
        "            blk.append(VioNetBlock(num_channels, kernel_size=kernel_size, use_1x1conv=(i == 0)))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Desain beberapa model VioNet dengan pelanggaran yang berbeda\n",
        "# (depth, num_channels, kernel_size) per stage\n",
        "vioNet1 = VioNet([(4, 32, 3), (2, 64, 5), (6, 128, 1), (2, 256, 3)], stem_channels=16)\n",
        "vioNet2 = VioNet([(3, 32, 1), (4, 80, 3), (5, 160, 5), (3, 320, 3)], stem_channels=32)\n",
        "\n",
        "# Cek model\n",
        "summary(vioNet1, (3, 224, 224))\n",
        "summary(vioNet2, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6d7cBubSaV5",
        "outputId": "8f5ab3d8-ae86-4ac1-9639-5121d24bc2c2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             448\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "              ReLU-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4         [-1, 32, 112, 112]             544\n",
            "            Conv2d-5         [-1, 32, 112, 112]           9,248\n",
            "       BatchNorm2d-6         [-1, 32, 112, 112]              64\n",
            "            Conv2d-7         [-1, 32, 112, 112]           9,248\n",
            "       BatchNorm2d-8         [-1, 32, 112, 112]              64\n",
            "            Conv2d-9         [-1, 32, 112, 112]           1,056\n",
            "      VioNetBlock-10         [-1, 32, 112, 112]               0\n",
            "           Conv2d-11         [-1, 32, 112, 112]           9,248\n",
            "      BatchNorm2d-12         [-1, 32, 112, 112]              64\n",
            "           Conv2d-13         [-1, 32, 112, 112]           9,248\n",
            "      BatchNorm2d-14         [-1, 32, 112, 112]              64\n",
            "      VioNetBlock-15         [-1, 32, 112, 112]               0\n",
            "           Conv2d-16         [-1, 32, 112, 112]           9,248\n",
            "      BatchNorm2d-17         [-1, 32, 112, 112]              64\n",
            "           Conv2d-18         [-1, 32, 112, 112]           9,248\n",
            "      BatchNorm2d-19         [-1, 32, 112, 112]              64\n",
            "      VioNetBlock-20         [-1, 32, 112, 112]               0\n",
            "           Conv2d-21         [-1, 32, 112, 112]           9,248\n",
            "      BatchNorm2d-22         [-1, 32, 112, 112]              64\n",
            "           Conv2d-23         [-1, 32, 112, 112]           9,248\n",
            "      BatchNorm2d-24         [-1, 32, 112, 112]              64\n",
            "      VioNetBlock-25         [-1, 32, 112, 112]               0\n",
            "           Conv2d-26         [-1, 64, 112, 112]           2,112\n",
            "           Conv2d-27         [-1, 64, 112, 112]         102,464\n",
            "      BatchNorm2d-28         [-1, 64, 112, 112]             128\n",
            "           Conv2d-29         [-1, 64, 112, 112]         102,464\n",
            "      BatchNorm2d-30         [-1, 64, 112, 112]             128\n",
            "           Conv2d-31         [-1, 64, 112, 112]           4,160\n",
            "      VioNetBlock-32         [-1, 64, 112, 112]               0\n",
            "           Conv2d-33         [-1, 64, 112, 112]         102,464\n",
            "      BatchNorm2d-34         [-1, 64, 112, 112]             128\n",
            "           Conv2d-35         [-1, 64, 112, 112]         102,464\n",
            "      BatchNorm2d-36         [-1, 64, 112, 112]             128\n",
            "      VioNetBlock-37         [-1, 64, 112, 112]               0\n",
            "           Conv2d-38        [-1, 128, 112, 112]           8,320\n",
            "           Conv2d-39        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-40        [-1, 128, 112, 112]             256\n",
            "           Conv2d-41        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-42        [-1, 128, 112, 112]             256\n",
            "           Conv2d-43        [-1, 128, 112, 112]          16,512\n",
            "      VioNetBlock-44        [-1, 128, 112, 112]               0\n",
            "           Conv2d-45        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-46        [-1, 128, 112, 112]             256\n",
            "           Conv2d-47        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-48        [-1, 128, 112, 112]             256\n",
            "      VioNetBlock-49        [-1, 128, 112, 112]               0\n",
            "           Conv2d-50        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-51        [-1, 128, 112, 112]             256\n",
            "           Conv2d-52        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-53        [-1, 128, 112, 112]             256\n",
            "      VioNetBlock-54        [-1, 128, 112, 112]               0\n",
            "           Conv2d-55        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-56        [-1, 128, 112, 112]             256\n",
            "           Conv2d-57        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-58        [-1, 128, 112, 112]             256\n",
            "      VioNetBlock-59        [-1, 128, 112, 112]               0\n",
            "           Conv2d-60        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-61        [-1, 128, 112, 112]             256\n",
            "           Conv2d-62        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-63        [-1, 128, 112, 112]             256\n",
            "      VioNetBlock-64        [-1, 128, 112, 112]               0\n",
            "           Conv2d-65        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-66        [-1, 128, 112, 112]             256\n",
            "           Conv2d-67        [-1, 128, 112, 112]          16,512\n",
            "      BatchNorm2d-68        [-1, 128, 112, 112]             256\n",
            "      VioNetBlock-69        [-1, 128, 112, 112]               0\n",
            "           Conv2d-70        [-1, 256, 112, 112]          33,024\n",
            "           Conv2d-71        [-1, 256, 112, 112]         590,080\n",
            "      BatchNorm2d-72        [-1, 256, 112, 112]             512\n",
            "           Conv2d-73        [-1, 256, 112, 112]         590,080\n",
            "      BatchNorm2d-74        [-1, 256, 112, 112]             512\n",
            "           Conv2d-75        [-1, 256, 112, 112]          65,792\n",
            "      VioNetBlock-76        [-1, 256, 112, 112]               0\n",
            "           Conv2d-77        [-1, 256, 112, 112]         590,080\n",
            "      BatchNorm2d-78        [-1, 256, 112, 112]             512\n",
            "           Conv2d-79        [-1, 256, 112, 112]         590,080\n",
            "      BatchNorm2d-80        [-1, 256, 112, 112]             512\n",
            "      VioNetBlock-81        [-1, 256, 112, 112]               0\n",
            "AdaptiveAvgPool2d-82            [-1, 256, 1, 1]               0\n",
            "          Flatten-83                  [-1, 256]               0\n",
            "           Linear-84                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 3,183,018\n",
            "Trainable params: 3,183,018\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 831.47\n",
            "Params size (MB): 12.14\n",
            "Estimated Total Size (MB): 844.19\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 112, 112]             896\n",
            "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
            "              ReLU-3         [-1, 32, 112, 112]               0\n",
            "            Conv2d-4         [-1, 32, 112, 112]           1,056\n",
            "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
            "            Conv2d-6         [-1, 32, 112, 112]           1,056\n",
            "       BatchNorm2d-7         [-1, 32, 112, 112]              64\n",
            "            Conv2d-8         [-1, 32, 112, 112]           1,056\n",
            "       VioNetBlock-9         [-1, 32, 112, 112]               0\n",
            "           Conv2d-10         [-1, 32, 112, 112]           1,056\n",
            "      BatchNorm2d-11         [-1, 32, 112, 112]              64\n",
            "           Conv2d-12         [-1, 32, 112, 112]           1,056\n",
            "      BatchNorm2d-13         [-1, 32, 112, 112]              64\n",
            "      VioNetBlock-14         [-1, 32, 112, 112]               0\n",
            "           Conv2d-15         [-1, 32, 112, 112]           1,056\n",
            "      BatchNorm2d-16         [-1, 32, 112, 112]              64\n",
            "           Conv2d-17         [-1, 32, 112, 112]           1,056\n",
            "      BatchNorm2d-18         [-1, 32, 112, 112]              64\n",
            "      VioNetBlock-19         [-1, 32, 112, 112]               0\n",
            "           Conv2d-20         [-1, 80, 112, 112]           2,640\n",
            "           Conv2d-21         [-1, 80, 112, 112]          57,680\n",
            "      BatchNorm2d-22         [-1, 80, 112, 112]             160\n",
            "           Conv2d-23         [-1, 80, 112, 112]          57,680\n",
            "      BatchNorm2d-24         [-1, 80, 112, 112]             160\n",
            "           Conv2d-25         [-1, 80, 112, 112]           6,480\n",
            "      VioNetBlock-26         [-1, 80, 112, 112]               0\n",
            "           Conv2d-27         [-1, 80, 112, 112]          57,680\n",
            "      BatchNorm2d-28         [-1, 80, 112, 112]             160\n",
            "           Conv2d-29         [-1, 80, 112, 112]          57,680\n",
            "      BatchNorm2d-30         [-1, 80, 112, 112]             160\n",
            "      VioNetBlock-31         [-1, 80, 112, 112]               0\n",
            "           Conv2d-32         [-1, 80, 112, 112]          57,680\n",
            "      BatchNorm2d-33         [-1, 80, 112, 112]             160\n",
            "           Conv2d-34         [-1, 80, 112, 112]          57,680\n",
            "      BatchNorm2d-35         [-1, 80, 112, 112]             160\n",
            "      VioNetBlock-36         [-1, 80, 112, 112]               0\n",
            "           Conv2d-37         [-1, 80, 112, 112]          57,680\n",
            "      BatchNorm2d-38         [-1, 80, 112, 112]             160\n",
            "           Conv2d-39         [-1, 80, 112, 112]          57,680\n",
            "      BatchNorm2d-40         [-1, 80, 112, 112]             160\n",
            "      VioNetBlock-41         [-1, 80, 112, 112]               0\n",
            "           Conv2d-42        [-1, 160, 112, 112]          12,960\n",
            "           Conv2d-43        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-44        [-1, 160, 112, 112]             320\n",
            "           Conv2d-45        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-46        [-1, 160, 112, 112]             320\n",
            "           Conv2d-47        [-1, 160, 112, 112]          25,760\n",
            "      VioNetBlock-48        [-1, 160, 112, 112]               0\n",
            "           Conv2d-49        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-50        [-1, 160, 112, 112]             320\n",
            "           Conv2d-51        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-52        [-1, 160, 112, 112]             320\n",
            "      VioNetBlock-53        [-1, 160, 112, 112]               0\n",
            "           Conv2d-54        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-55        [-1, 160, 112, 112]             320\n",
            "           Conv2d-56        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-57        [-1, 160, 112, 112]             320\n",
            "      VioNetBlock-58        [-1, 160, 112, 112]               0\n",
            "           Conv2d-59        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-60        [-1, 160, 112, 112]             320\n",
            "           Conv2d-61        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-62        [-1, 160, 112, 112]             320\n",
            "      VioNetBlock-63        [-1, 160, 112, 112]               0\n",
            "           Conv2d-64        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-65        [-1, 160, 112, 112]             320\n",
            "           Conv2d-66        [-1, 160, 112, 112]         640,160\n",
            "      BatchNorm2d-67        [-1, 160, 112, 112]             320\n",
            "      VioNetBlock-68        [-1, 160, 112, 112]               0\n",
            "           Conv2d-69        [-1, 320, 112, 112]          51,520\n",
            "           Conv2d-70        [-1, 320, 112, 112]         921,920\n",
            "      BatchNorm2d-71        [-1, 320, 112, 112]             640\n",
            "           Conv2d-72        [-1, 320, 112, 112]         921,920\n",
            "      BatchNorm2d-73        [-1, 320, 112, 112]             640\n",
            "           Conv2d-74        [-1, 320, 112, 112]         102,720\n",
            "      VioNetBlock-75        [-1, 320, 112, 112]               0\n",
            "           Conv2d-76        [-1, 320, 112, 112]         921,920\n",
            "      BatchNorm2d-77        [-1, 320, 112, 112]             640\n",
            "           Conv2d-78        [-1, 320, 112, 112]         921,920\n",
            "      BatchNorm2d-79        [-1, 320, 112, 112]             640\n",
            "      VioNetBlock-80        [-1, 320, 112, 112]               0\n",
            "           Conv2d-81        [-1, 320, 112, 112]         921,920\n",
            "      BatchNorm2d-82        [-1, 320, 112, 112]             640\n",
            "           Conv2d-83        [-1, 320, 112, 112]         921,920\n",
            "      BatchNorm2d-84        [-1, 320, 112, 112]             640\n",
            "      VioNetBlock-85        [-1, 320, 112, 112]               0\n",
            "AdaptiveAvgPool2d-86            [-1, 320, 1, 1]               0\n",
            "          Flatten-87                  [-1, 320]               0\n",
            "           Linear-88                   [-1, 10]           3,210\n",
            "================================================================\n",
            "Total params: 12,616,906\n",
            "Trainable params: 12,616,906\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 1160.69\n",
            "Params size (MB): 48.13\n",
            "Estimated Total Size (MB): 1209.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Your goal is to design the perfect MLP. Can you use the design principles introduced above to find good architectures? Is it possible to extrapolate from small to large networks?"
      ],
      "metadata": {
        "id": "C4bg9DGLSuHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mulai dengan arsitektur dasar:\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 2. Tambahkan kedalaman dan regularization:\n",
        "\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, depth):\n",
        "        super(DeepMLP, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "            self.layers.append(nn.Dropout(p=0.5))  # Regularization\n",
        "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(layer(x))\n",
        "        return x\n",
        "\n",
        "# 3. Eksperimen dengan skip connections:\n",
        "\n",
        "class SkipMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, depth):\n",
        "        super(SkipMLP, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = x\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(layer(x))\n",
        "        return x + skip  # Skip connection"
      ],
      "metadata": {
        "id": "_4BZMh-ETj66"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mengekstrapolasi dari network kecil ke network yang lebih besar tidak hanya memungkinkan, tetapi juga merupakan strategi praktis dalam deep learning. Dengan memanfaatkan wawasan yang diperoleh dari model yang lebih kecil, seseorang dapat merancang arsitektur yang lebih besar yang mempertahankan kekuatan dari rekan-rekan mereka yang lebih kecil sambil meningkatkan kapasitas dan performa pada tugas yang kompleks.\n",
        "\n",
        "Namun, pertimbangan yang cermat terhadap hyperparameter, strategi pelatihan, dan ketersediaan data sangat penting untuk keberhasilan penskalaan."
      ],
      "metadata": {
        "id": "M8J6I9NSWTs_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgzV+oaWwTaaW7TiT1Ay5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}